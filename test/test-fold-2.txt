While working on desktops or laptops, itâ€™s useful to have a second display handy. Another monitor can easily be plugged in, but why not use the screens you already have instead of going off and purchasing another one? With a little bit of effort, the iPad, iPhone, and iPod Touch can be turned into a second screen for your Mac or Windows computer. This can be used in two ways: using the iOS device as a true second display, or mirroring the content of your main display. In this post, youâ€™ll learn how to accomplish both.Air Display is actually two separate applications: one running in Windows or OS X, and an app running in iOS. First off, download and install the client on your computer. Next, youâ€™ll need to buy the iOS app for $9.99 on the App Store. Once both are running, connect both to the same WiFi network, and then follow the on-screen instructions to make sure they are talking to each other.Once theyâ€™re connected on OS X, you can configure them even further. Launch System Preferences, and select the Displays section. On the iOS device, youâ€™ll be able to configure your resolution. Specifically, devices with Retina displays can enable High Dots Per Inch (HiDPI) mode that draws windows as if the resolution was a quarter the size, but with the full detail that your screen allows. Sadly, the Windows version doesnâ€™t support the high resolution mode properly just yet, but the developer promises this will be added in a future update.On your main display in Mac OS X, you can now adjust where the second screen sits in relation to your main screen by switching to the â€œArrangementâ€? tab. In Windows, you need to open the system tray icon and select â€œDisplay Arrangement.â€? In OS X, this screen also displays a toggle called â€œMirror Displays.â€? This will turn your iOS device into a duplicate of your main screen. This is useful if youâ€™re trying to show someone a website or a photo, and you donâ€™t want to huddle in front of your computer. If you want to enable mirroring on Windows, simple click the system tray icon, and navigate to â€œOptions,â€? and click â€œEnable Mirror Mode.â€?If all youâ€™re looking for is display mirroring and control of your main screen on your iOS device, Virtual Network Computing (VNC) might be the best option for you. This is built right into Mac OS X, but Windows users will need something like TightVNC which is available for free. Not only will VNC mirror your screen, but it also allows you to control the computer remotely. Seeing whatâ€™s on your screen is nice, but being able to manipulate your computer when youâ€™re not at your desk is even better.To turn it on in OS X, go into System Preferences under the Sharing section, and check the Screen Sharing toggle. Whichever way you plan on enabling VNC on your computer, make note of your local IP or Bonjour address displayed by your software.Next, youâ€™re going to need a VNC client for your iOS device. Some clients are available for free like Mocha VNC Lite, but apps like iTeleport ($4.99) and Mocha VNC ($5.99) are more feature-rich. Once you have one installed, input your computerâ€™s IP or Bonjour address into the configuration, and youâ€™re good to go.These require a bit of effort to set up, but it certainly worth the hassle. Youâ€™ll be glad to have went jumped through these hoops when you need that display at your desk, or if you want to turn off that download without getting out of bed.PURDUE (US) â€” Objects created with 3D printing often fall apart or lose their shape, but new software anticipates weak spots and increases durability.â€œI have an entire zoo of broken 3D printed objects in my office,â€? says Bedrich Benes, an associate professor of computer graphics at Purdue University.The printed fabrications often fail at points of high stress.â€œYou can go online, create something using a 3D printer and pay $300, only to find that it isnâ€™t strong enough to survive shipping and arrives in more than one piece,â€? says Radomir Mech, senior research manager from Adobeâ€™s Advanced Technology Labs.The 3D printers create shapes layer-by-layer out of various materials, including metals and plastic polymers. Whereas industry has used 3D printing in rapid prototyping for about 15 years, recent innovations have made the technology practical for broader applications, he says.â€œNow 3D printing is everywhere,â€? Benes says. â€œImagine you are a hobbyist and you have a vintage train model. Parts are no longer being manufactured, but their specifications can be downloaded from the Internet and you can generate them using a 3D printer.â€?The recent rise in 3D printing popularity has been fueled by a boom in computer graphics and a dramatic reduction of the cost of 3D printers, Benes says.Researchers at Purdue University and Adobeâ€™s Advanced Technology Labs have jointly developed a program that automatically imparts strength to objects before they are printed.â€œIt runs a structural analysis, finds the problematic part and then automatically picks one of the three possible solutions,â€? Benes says.The researchers detailed their findings in a paper presented during the SIGGRAPH 2012 conference in August.Former Purdue doctoral student Ondrej Stava created the software application, which automatically strengthens objects either by increasing the thickness of key structural elements or by adding struts. The tool also uses a third option, reducing the stress on structural elements by hollowing out overweight elements.â€œWe not only make the objects structurally better, but we also make them much more inexpensive,â€? Mech says. â€œWe have demonstrated a weight and cost savings of 80 percent.â€?The new tool automatically identifies â€œgrip positionsâ€? where a person is likely to grasp the object. A â€œlightweight structural analysis solverâ€? analyzes the object using a mesh-based simulation. It requires less computing power than traditional finite-element modeling tools, which are used in high-precision work such as designing jet engine turbine blades.â€œThe 3D printing doesnâ€™t have to be so precise, so we developed our own structural analysis program that doesnâ€™t pay significant attention to really high precision,â€? Benes says.The paper was authored by Stava, now a computer scientist at Adobe, doctoral student Juraj Vanek; Benes; Mech; and Nathan Carr, a principal scientist at Adobeâ€™s Advanced Technology Labs.Future research may focus on better understanding how structural strength is influenced by the layered nature of 3D-printed objects. The researchers may also expand their algorithms to include printed models that have moving parts.It's a big storm, moving slowly. A gigantic span of ferocious swirl meets a front of chilly resistance. The effect of that collision is amplified by powerful tidal influence. Upheavals and surges swamp the landscape. Many people are displaced; countless others stay with the familiar.Also, in the real world, some nasty weather is happening. But I'm talking about the tech industry of the last five business days, which has aligned and concentrated its forces in a crystal-clear demonstration, if one were needed, that mobile is where the bets are placed and futures will be won and lost.Apple is at the eye of the storm, where its devoted legions expect it, but no longer as a pioneer. Defending its territory rather than breaking new ground, the post-Jobs company did something its late and fabled leader scorned, split hairs to justify it, engaged in implicit combat with four competitors, ticked off some of its best customers and was squeezed by inexorable pressure of a quickly evolving industry.As I noted a week ago, it has been a perfect storm of product announcements and earnings releases. The two are always entwined. Though we like to imagine that companies are solely dedicated to the happiness of consumers at the end of the chain, the drumbeat of quarterly reports is what drives most decisions around product timing and the release of feature sets.This umbilical connection was etched in bold relief last week when Apple announced a new mini-maxi-Mac product lineup just two days before its Q4 earnings call. The mysteries of one were explained by the other.Though the accumulated import of last week's events had a tectonic rumble, there was really only one surprise -- the launch of a fourth-generation iPad, an upgrade that left disciples slack-jawed, and not entirely euphoric. Christina Warren spoke on behalf of incensed iPad owners in a 1,500-word rampage that explored the thesaurus entry for "angry" and invited rugged push-back of the #firstworldproblem type. The disaffected have a point, which is that a seven-month dev cycle (between the third- and fourth-gen iPads) is shorter than usual for Apple, and therefore, arguably, deceptive to third-gen buyers.The argument loses steam when you splash cold water on your throbbing veins and remember that Apple is a down-to-business corporation like any other, its steely eyes focused on managing its public stakeholders. That can be hard to remember during the live event, which is about shiny new features and end-use scenarios.In the earnings call two days later, CEO Tim Cook and CFO Peter Oppenheimer laid out past and future performance metrics like snapshots in a mosaic. Sales of iPads missed projections; iPod sales likewise below expectation. Mac down. Revenue flat against guidance, but earnings-per-share down. Most important to analysts: gross profit margin just below expectation, and projected to dip further. In fact, gross margins have skidded the last two quarters, from 47 percent to 40 percent, and the fiscal Q1 projection (that's the current quarter) is 36 percent, which harks back to fiscal 2008. Against all of this is a backdrop of plunging AAPL stock.When margins slump, volume must make up the difference. In a voracious market of technology adopters, sales come from new and refreshed products. Hence, the mini and the fourth-gen iPad.The post-PC company more clearly entered the post-Jobs epoch as Apple repudiated its previous scorn for small tablets. The mini is not a 7-incher! Tim Cook wants us to be clear on that point. It is a 7.9-incher. That's 35 percent bigger than a 7-inch screen! Put down the Red Bull, Tim, we get the point. But Cook also weirdly and defensively compared the iPad mini to the iPad 2 ("...equal to or better than the iPad 2 in every way"), and a portion of the commentariat complained that the specs were weak, barely competitive with the Google Nexus.The $329 starting price isn't earning many compliments either. This is how Apple works the offense and defense in the same play. Defensively, the company was forced to plug its portfolio gap with an intermediate slate. Forbes divined from Amazon's earnings call (also last week) that the Kindle Fire, with its succulent price point, is eating into iPad's share. Impossible to know for sure, since Amazon doesn't break out Kindle sales.But we know as a corollary that Samsung whipped Apple in smartphone share and units shipped in calendar Q3 by two to one. (56M vs. 27M phones sold; 31 percent vs. 15 percent share.) In a barbed announcement, Samsung noted that its Galaxy S III experienced a sales spike immediately after the iPhone 5 release.Google is another share-stealing tormentor. Sadly, Google scheduled its New York event in conflict with the latest storm of the century, and tiptoed out of the city when the weather forecast firmed up. But the new products came out today anyway: a 10-inch Nexus, plus a memory upgrade and price reduction of the 7-inch tablet line. In Apple's perspective: more pressure. The $329 iPad mini will soon be fighting for holiday gift status with a $200, 16GB 7-inch Nexus.For its offensive game, Apple relies on the concept and reality of premium. "Premium" means different things to different people, and its specifications are always changing. Apple's bankable status as a premium merchant has relied on build quality (still current), brand reputation (ephemeral, but earned and lasting for now), screen display quality (soon to be bettered by a new Nexus), and a safe, curated, huge app ecosystem (hanging onto leadership there for the time being).More than any other quality, though, Apple has accrued premium credibility through innovation leadership. Its destiny as a business titan depends on whether the company has invention left in the gas tank. Without the innovation, Apple is in an assembly-line business of iterating its products, synchronizing release cycles with finance milestones, managing its pipeline and massaging margins. Naturally, any company must do all these things. But last week we didn't see freshness from Apple; we saw a company loading up the pipeline for an earnings assault in Q1 with a barrage of products. In the live event, Tim Cook bragged about "...a truly prolific year for innovation for Apple." Prolific is not breakthrough. Apple did not become the world's most valuable company by making screens thinner, tablets smaller or phones longer. It got there by persuading society to adopt new categories.The dark-horse innovator last week was Microsoft, an aging legacy ruler facing entropic decay in a changing world of unmoored devices. There were no surprises; Microsoft held back no secrets about the radically different Windows 8 and the new Surface tablet. Windows Phone 8 was announced earlier today as the smartphone leg of Microsoft's stool. The boldness and commitment of Redmond's bet is breathtaking; that is universally recognized. But it's uphill for Microsoft's under-developed ecosystem: according to an Associated Press poll, most people haven't even heard of Windows 8.So it's Apple vs. Microsoft on daring, Apple vs. Samsung on smartphone market share and patent conflicts, Apple vs. Google on specifications and price, Apple vs. Amazon on willingness to cut margin, Apple vs. its customers on betrayed expectations, Apple vs. itself on the insurmountable challenge of remaining a true innovator forever. Interesting times. Fortunately for manufacturers and consumers both, it's not a winner-take-all industry. Get out the pie cutters.Brad Hill is a former Vice President at AOL, and the former Director and General Manager of Weblogs, Inc.I had a great idea this morning. I figured Iâ€™d head to the Microsoft Store in Scottsdale around 10am, waltz in, buy a Microsoft Surface, and then be out in 10 minutes. I assumed the store would be empty. I mean, come on, this is a Microsoft tablet weâ€™re talking about, and who goes to the Microsoft Store anyway?I was completely wrong.Â Microsoftâ€™s new tablet, the Surface RT, may not do everything an iPad can, but itâ€™s drawing some pretty big lines to Microsoft retail stores across the country for its launch this morning.Once I got to the Microsoft Store I was shocked to see a line of about 125 people waiting to buy the Surface and was told itâ€™d be a two hour wait before I could get in. The story is the same at other Microsoft Stores across the country with people lining up to purchase Microsoftâ€™s hyped tablet thatâ€™s supposed to compete with the iPad.The line at the Seattle stores have been reported to be the largest, but many people in those lines are associated with Microsoft. Microsoft isnâ€™t used to dealing with long launch lines though. A lot of people on Twitter have complained that itâ€™s taking Microsoft retail employees an hour to to get 15 people or so through the line. Apple usually churns through about one hundred or so customers on launch day every hour.Whether clinging to the inside of a bell jar, or outstretched from floor to ceiling, San Francisco-based artist Dan Grayberâ€™s mechanisms have but one purpose: to stay upright. Combining pulleys, bike brake cables, counterweights, and steel framing, Dan writes,Many of my pieces are small, spring loaded, mechanical objects. They are intricately designed and fabricated to accomplish one of the most simple, yet most essential tasks that an autonomous object can. This task, this need, is that of holding itself up. In most cases, my pieces accomplish this by actively attaching themselves to specific architectural features and individual objects.Some of Danâ€™s mechanisms self-install on walls, creating holes like industrial tracks. Others cling to corners or specific architectural features. More artworks can be seen in Grayberâ€™s online portfolio.One year ago, EFF rang alarm bells about SOPA and PIPA â€” Internet censorship bills threatening online freedom and the very structure of the Internet. Not long after, our members and the Internet community stood up to misguided politicians and deep-pocketed lobbyists and won, sinking SOPA and PIPA forever. It was revolutionary.You have the power to shape this world, and EFF stands with you. For 22 years, donating members have enabled EFF to bring legal and technological expertise into crucial battles about online rights, from defending free speech online to challenging unconstitutional surveillance.Your participation makes a difference, and weâ€™re proud to be a member-supported organization. Please consider becoming an EFF member today â€” every donation, no matter the size, guarantees that we who value freedom online will always have a voice and a formidable advocate.Help protect the free and open Internet. Join or renew your membership with the Electronic Frontier Foundation today!The prospect of growing crops in vertical farms directly inside of cities has been on the collective wish-list of environmentalists, sustainable developers, and futurists for quite some time now. And now it looks like it's finally starting to happen. Land-strapped Singapore has opened its first vertical farm â€” an innovation that will increase the variety of foods it has available and decrease its dependance on foreign imports.And indeed, a major problem facing Singapore (and many other cities) today is land scarcity. Located at the tip of the Malay Peninsula, it is an island country that consists of a mere 710 square kilometers (271 square miles) â€” and most of it is developed and urbanized. Today, only 7% of Singapore's vegetables are grown locally. But by virtue of the new facility, it's looking to change the situation.Developed by Sky Greens Farms, the vertical farm consists of 120 aluminum towers that extend over 9 meters (30 feet) in height. In total, the vertical farm is able to produce vegetables at a rate of 0.5 tonnes per day. The company is hoping to attract investors so that it can devote another USD$21M dollars for upgrades. Ideally, they'd like to construct as many as 300 towers â€” enough to produce two tonnes of vegetables per day.Currently, the farm is able to grow three kinds of vegetables, and they can only be found at the local FairPrice Finest supermarkets, but at a price that's 10 to 20 cents more than vegetables from other sources. But according to Channel News Asia, customers are enthusiastic about the new products and the supermarkets are struggling to keep the vegetables in stock. Moreover, Sky Greens expects the price to drop as the farm ramps up supply.The FSF has fought for years against the threat of Digital Restrictions Management (DRM). Users should have the right to modify, share and learn from the software on their devices, and technical measures put in place in the name of DRM offer a substantial roadblock. It's even worse when those measures have the force of criminal law behind them, threatening people who simply want to change the software on their computers with jail time. The FSF wants to create a world in which there is no DRM. Until then, at the very least, users shouldn't have to worry about legal consequences for disabling these malfeatures on their own devices.The Digital Millennium Copyright Act (DMCA) of course circumvents the rights of users by making it illegal to modify your devices in ways that would give you actual access to them, or to share tools to help others do this. Congress did create one small carve-out from this belligerence; that once every three years the Library of Congress (via the Copyright Office), would consider making exceptions to this broad rule. In 2010, the Office recommended exempting the freeing of cellphones. They did not, however, make clear that this exemption extended to people who distributed tools for freeing these devices. In 2012, we had hoped to expand the exempted class of uses, and encouraged the Copyright Office to extend exemptions to tablets, gaming consoles, and computers running restricted boot. We were on the side of organizations like the EFF, and the Mozilla Foundation as well as hundreds of other individuals calling for the protection of those who simply want to be able to use their own devices in freedom.But we were not the only ones to send recommendations to the Copyright Office. Large corporations like Sony, and corporate-backed groups like "Joint Creators and Copyright Holders" also sent comments opposing these reasonable exemptions. And the Copyright Office fell for their FUD. The Copyright Office has announced that while freeing your phone in order to install your own software is still permitted, unlocking the phone in order to switch carriers will be phased out. And even that minimal remaining protection has not been extended to tablets. Offering the duplicitous explanation that they weren't sure what a tablet was, the office completely abdicated its responsibility to protect users' rights to run their own software on their devices, as well as their rights to works locked down on those tablets. They similarly rejected exemptions for users wanting to install their own operating system on game consoles, and even worse, failed to extend protection to users who want to install their own operating system on computers with restricted boot.This means no longer being able to switch your own cell phone carrier without permission. This means no modifying tablet operating systems without legal threat. It means that trying to install a different operating system on your game console could result in the FBI breaking down your door. It means that you cannot even be sure of your right to remove proprietary software from devices encumbered with restricted boot.The Copyright Office picked Sony over you. They had an opportunity to protect users, but instead chose to protect corporate interests. This is a terrible outcome for users everywhere, and just proves that we need wholesale elimination of the anti-circumvention laws.We need to band together. Here is what you can do to help:South Carolinaâ€™s Department of Revenueâ€™s computer system was hacked, resulting in the compromise of some 3.6 million Social Security numbers on top of nearly 400,000 credit and debit card numbers exposed. The actual breaches occurred in September and October.Adding insult to injury, none of the Social Security numbers were encrypted; nor were 16,000 credit card numbers. The South Carolina Division of Information Technology apparently informed the Department of Revenue of the breach Oct. 10, according to local news reports. Anyone who filed a tax return in South Carolina after 1998 is urged to call 866-578-5422.â€œThis is not a good day for South Carolina,â€? governor Nikki Haley told an Oct. 26 press conference, according to WACH Fox News Center, adding about the hacker responsible: â€œI want this person slammed against the wall.â€?On Oct. 26, Haley filed an executive order to beef up the stateâ€™s security. â€œI hereby direct all cabinet agencies to immediately designate an information technology officer,â€? it read, â€œto cooperate with the State Inspector General who is authorized to make recommendations to improve information security policies and procedures in state agencies.â€?The order also stipulates cooperation with national cyber-security sources such as the Sharing Analysis Center, collaboration with in-state agencies to identify vulnerable points in cyber-security systems, and improvement in the training of government employees in information security measures.In the meantime, the current breach is under intense investigation by state authorities. Various local news sources are reporting that the attack came from a â€œforeign country.â€?According to Census.gov, the population estimate for the state of South Carolina is a bit over 4.67 million souls, meaning that roughly three-quarters of its citizensâ€™ Social Security numbers are in the hands of hackers.â€œFrom the first moment we learned of this, our top priority has been to protect the taxpayers and the citizens of South Carolina, and every action weâ€™ve taken has been consistent with that priority,â€? South Carolina DOR director James Etter wrote in an Oct. 26 statement. â€œWe have an obligation to protect the personal information entrusted to us, and we are redoubling our efforts to meet that obligation.â€?To reduce online piracy, Google has implemented several changes to its search engine in recent years. Among other things, Google has blacklisted dozens of piracy related terms from appearing in its autocomplete and instant services. Megaupload is one of these search terms, and nine months after the last infringement took place the name of Kim Dotcomâ€™s file-hosting service is still being censored. This begs the question, what other terms are needlessly censored by Googleâ€™s blacklist?Since January 2011, Google has been filtering â€œpiracy-relatedâ€? terms from its â€˜Autocompleteâ€˜ and â€˜Instantâ€˜ services.Google users searching for terms like â€œtorrentâ€?, â€œBitTorrentâ€? and â€œMegauploadâ€? will notice that no suggestions and search results appear before they type the full word. While no search results are removed from Googleâ€™s index, there is sharp decrease in searches for these terms.What triggers a keyword to be included in the blacklist is not clear, but a Google spokesperson previously told TorrentFreak that they remove terms that are â€œclosely associated with infringing results.â€?â€œItâ€™s not easy and the list will undoubtedly change over time. When evaluating terms for inclusion, we examine several factors, including correlation between the term and results that have been subject to valid DMCA takedown notices,â€? Google told us.Sounds deliberate, and as weâ€™ve documented in the past the list has indeed been changed. Many new terms have been added since the start, most recently to include several of The Pirate Bayâ€™s domain names.However, these changes appear to go only one way.Megaupload, for example, is still among the censored terms even though the site has been offline for more than nine months. There are simply no accurate â€œcopyright infringementâ€? grounds to keep it blacklisted, one would think.Nevertheless, searching for â€œMegauploaâ€? today still shows no Autocomplete and Instant results at all.Funnily enough, Googleâ€™s search algorithm bypasses the filter to some extent, suggesting â€œMegauplauploadâ€? when typing â€œMegauplâ€? and displaying instant results for a â€œMegauploadâ€? search because thatâ€™s a far more popular search term.That said, the concern remains that once a term is placed on Google piracy blacklist itâ€™s not so easy to get it taken off.At this point itâ€™s still unclear what factors dictate a term being placed on Googleâ€™s piracy blacklist. Itâ€™s also unknown how many piracy related terms are censored as the list is not made public.To get a better understanding of Megauploadâ€™s continued presence in the blacklist and what the update policy is, TorrentFreak asked Google for a comment, but we have yet to receive a reply.In the meantime we decided to compile our own list of terms that are currently blocked from Instant and Autocomplete. Most terms are related to torrent sites and cyberlockers. In many cases the full url is not blocked, which then simply takes over as an Autocomplete suggestion.Readers are welcome to add more censored keywords in the comment so we can add to the list.Status Symbols are devices that transcend their specs and features, and become something beautiful and luxurious in their own right. They're things that live on after the megapixel and megahertz wars move past them, beacons of timeless design and innovation.2005 was a good year for Nintendo handhelds. The original DS was on its way to becoming the most successful portable device of all time, while the Game Boy Advance SP let you play your entire Game Boy library â€” dating back to the monochromatic original â€” on one, handy machine. So it was a bit curious, then, when the company decided to release the $99 Game Boy Micro, a small, streamlined version of the handheld that could only play GBA games. It improved form at the expense of functionality, creating a device that wasn't strictly necessary, but was amazing anyways.The most important thing about the Micro was its size â€” it was downright miniscule. The screen was only two inches across and the entire thing weighed just 0.18 pounds. That's less than half the weight of the original Game Boy (0.49 pounds) and a drop even from the ultralight iPhone 5's 0.25 pounds. It was so small and light you could leave it in a bag â€” or even your pocket! â€” and forget it was there. But it wasn't just that the Micro was small, it was also stylish in a way no Nintendo device had ever been. Unlike the clunky DS or any version of the Game Boy or GBA, the Micro wasn't something you'd be embarrassed to pull out in public. It felt like a gadget, not a toy. The 20th anniversary edition was particularly lovely, with a gold and red color scheme reminiscent of the original Famicom controller (the Japanese version of the NES).It was stylish in a way no Nintendo device had ever beenWhile its size made it an ideal companion for just about any trip â€” I particularly enjoyed using it for grinding through Final Fantasy V levels in between, and occasionally during, university classes â€” the screen is what made the Micro a great game system. It was small, but it was beautiful. Shrinking down games made them appear crisper, and the brilliant backlight made older games pop with new life and color. You haven't played The Legend of Zelda: The Minish Cap until you've played it on a Micro with the brightness cranked up to 11. And if you wanted to feel extra cool, the Micro was ideal for playing the Japan-exclusive Bit Generations line of GBA games â€” sleek, minimalist games in sleek, minimalist packaging, just begging to be played on a sleek, minimalist Game Boy.Like many beautiful devices, the Micro also had its share of problems. The smaller screen wasn't ideal for text-heavy games, the faceplate was prone to scratches, and the ergonomics could feel a tad cramped after lengthy sessions. But sometimes you have to make sacrifices, and with the Micro it was more than worth it. The combination of its size, style, and screen made it the first machine from Nintendo that looked as good as it played. And unlike later releases, like the iPod-influenced DS Lite, the Micro had a look all its own, and one that has yet to be duplicated. It was the last device to feature the Game Boy name, and though it was far from the most popular, it was definitely the coolest.Do not buy a Microsoft Surface RT yet.Iâ€™m typing this with gritted teeth.Â  My 24 hours with the half-baked Surface have been a frustrating challenge, a mix of love and hate.Â  I want want want this to work, but one problem after another have led me to come to the conclusion â€“ a temporary one at least â€“ that this thing just isnâ€™t ready to ship.Every time Apple unveils a new gadget or laptop, my jaw drops and I wonder how they pulled off executing their industrial designs.Â  Their v1 designs look so beautifully put together, not a mishmash of plastic parts and lids like the PC counterparts.Â  Every now and then, a PC maker will bring out something similar, but itâ€™s the very rare exception rather than the rule.The Surface RT is Microsoft shoving their hardware partners aside and saying, â€œLemme show you how this should be done. Pay attention, kids.â€?This tablet hardware doesnâ€™t just compete with the iPad â€“ it bypasses the iPad in many ways that are significant and valuable for me.I plugged in my USB presentation remote and it just worked.I plugged in a 64GB micro SD card with all my presentations and files and it just worked.I popped out the kickstand and started typing and it just worked.Â  Well, almost â€“ if thereâ€™s one significant compromise in the Surface RT, itâ€™s the kickstand.Â  You get two and only two positions for the kickstand: open and closed.Â  Thereâ€™s no adjustments.Â  I think the kickstand angle was designed for airplane use by short people, because the screen hardly goes back at all.Â  Itâ€™s probably perfect for Danny DeVito when he puts it on the seat back tray in coach class, but for me on a desk, itâ€™s too steep.The built-in front-facing camera for Skype is angled so that itâ€™ll work great when the kickstand is open, but again, only for Danny DeVito, or maybe for people who want to show off their chests in Skype.There are other hardware compromises, but theyâ€™re pretty small.Â  The speakers are laughably quiet; I fired up one of my favorite movies, Once Upon a Time in Mexico, and I couldnâ€™t even hear the actorsâ€™ dialog in the opening scenes.Â  Not couldnâ€™t understand â€“ couldnâ€™t even hear it.Â  The magnetic power cord doesnâ€™t snap in with authority, but rather requires careful positioning.Â  The volume up/down buttons are exactly opposite the USB port, so when I plug in USB devices I often push the volume up/down by accident.But who cares? I HAVE A USB PORT! Oh, Steve Jobs, I understand that you were a design deity, but I really needed that USB port, and I didnâ€™t want a stupid dongle to get it.Â  The iPad has a USB dongle available, but it was useless to me because I needed it for my presentation clicker at the same time I also needed video out, but I couldnâ€™t use both simultaneously.The Type Cover (the one with real keys) just works.Â  Iâ€™ve got big hands that often struggle on undersized keyboards, but I can type very quickly on the Type Cover.Â  So quickly, in fact, that I can outrun Microsoft Word on the Surface.Â  I get the feeling that the Surface RTâ€™s CPU or Word code just canâ€™t keep up with my typing.Â  Hereâ€™s an example video:But thatâ€™s not a hardware problem â€“ and itâ€™s time for us to talk about the ugly problem with the Surface RT.The hardware makes promises that the software canâ€™t deliver â€“ and the ability to type faster than Word can digest is a great example of that.Â  Sure, I understand that the shipped version is â€œMicrosoft Word Preview,â€? but you canâ€™t deliver software like this.Â  Itâ€™s a recipe for returned products â€“ and frankly, thatâ€™s exactly what Iâ€™m going to do with the Surface RT, return it.Wordâ€™s problems arenâ€™t limited to slow typing.Â  Once youâ€™ve banged out a document, saving your work is another adventure:I can understand problems with Word because itâ€™s a new piece of software that Microsoft has never released bef â€“ wait, hold on. Iâ€™m being told by my staff that Word is not a new program, and has been out since the 1980s.Â  If I want to see a v1 program, theyâ€™re telling me to look at the Mail app.Â  Alright, letâ€™s give that a shot:After waiting over a minute for the machine to boot and launch the mail app, I got a blank gradient screen. User interface 101: if the app needs to be set up on the first launch, offer to do that, please.Â  Folks from Twitter suggested that I swipe out from the right side and click Accounts, Add, and I did, but the Surface just sat there as shown in the video.Â  Eventually, after setting the unit aside and going on with my day, I noticed several minutes later that it popped up and said it couldnâ€™t detect the email servers for brento@brentozar.com.Â  User interface 102: when youâ€™re doing something, say something.The Surface Pro comes out in a few months.Â  The hardware design is very similar, but heavier, thicker, and with a â€œrealâ€? processor that requires a fan.Â  Yes, those are drawbacks, but they come with a very, very powerful advantage: the Surface Pro will run real Windows 8.Â  This means (hopefully) none of the buggy Windows RT problems, and perhaps more importantly, a full stable of applications.See, the Surface RT only runs Metro (whatever) apps, of which there are woefully few.Â  I didnâ€™t even get to the point of testing the very few that I found â€“ forget it, because the built-in stuff is so incredibly bad.Â  The lack of apps wasnâ€™t a problem for me â€“ I explained why I preordered a Surface RT â€“ but the quality of the built-in apps was.The whole point of the Surface RT was supposed to be a tablet thatâ€™s ready for work.Â  Itâ€™s not.Â  Donâ€™t touch it.After getting linked from HN and Reddit, Iâ€™ve gotten a bazillion comments that boil down to â€œYou should have updated Office.â€?Â  Yes, if only I could have figured out how.Â  Since this post went live, Microsoft has explained how to get it:For Windows RT Surface users, the update can be had by:Emphasis mine.Â  I had no idea that there were multiple places for Windows Update on the same tablet.Â  One tablet, but multiple places to get Microsoft updates?Â  And weâ€™re not even counting the Windows Store here.Â  This just isnâ€™t realistic to expect end users to find this buried treasure.Other commenters have suggested that the Office updates apply automatically overnight â€“ they do not.Â  Iâ€™d left my Surface RT plugged in overnight, but even so, that only lets automatic updates apply, not optional ones like this Office update.And of course, keep in mind that I still donâ€™t know if these updates fix the problem â€“ they certainly donâ€™t fix the camera or mail problems, both of which were already updated through Windows Update.Yesterday this got posted to a bunch of news sites. I was out shopping with Erika when I got a tweet saying Iâ€™d hit the front page of HackerNews, LoopInsight, and Reddit, plus getting linked to from comments at CNet and Techmeme.Hereâ€™s what that looks like in Google Analytics:Yesterday was supposed to be a fun shopping day, just Erika and I out looking at furniture and clothes before my trip out to DevConnections and the PASS Summit. Increasingly, though, I kept turning to my phone and typing frantically, trying to explain things to commenters. My stress level went through the roof, and eventually I realized that being out and about was probably the best thing that could happen. I stopped trying to keep up, and just went back to my life â€“ taking Ernie for a long walk, going out for dinner, reading the paper.Yesterday was frustrating as all hell.Iâ€™m a geek. Iâ€™ve been using computers since my first Commodore 64, then writing code in Topspeed Clarion, VBscript, Java, and .NET before switching over to Microsoft SQL Server database administration. I know bugs. Iâ€™ve coded bugs. (Thatâ€™s probably all Iâ€™ve ever coded, come to think of it.) Iâ€™m used to poking around to discover workarounds to get things to work. Iâ€™m very used to doing updates to devices before I start working with â€˜em, and I repeatedly did updates on the Surface RT trying to get it to work.Iâ€™m not a zealot. I use both Microsoft and Apple gear, and while a lot of my SQL Server friends rant against cloud-based and NoSQL databases, I like those too. Iâ€™m all about using whatever works best â€“ or to be more specific, whatever sucks the least. No software or hardware is perfect, although Iâ€™ll be the first to tell you that the Surface RTâ€™s hardware comes pretty darned close to being perfect for 2012 tablets. The iPad isnâ€™t. I hate that Apple continues to burden their products with wacko connectors, and now theyâ€™re even changing the connectors. Give me a freakinâ€™ USB port, memory card port, and video out port, and letâ€™s call it a day.I really, really wanted the Surface RT to work. I need a lightweight backup PowerPoint device when Iâ€™m on the road presenting at conferences. That device needs to show PowerPoint presenter view while driving an external projector, while being plugged in for electricity (some of my sessions are 8-9 hours long), and take a presentation clicker. Keynote Remote doesnâ€™t cut it because it loses reception in noisy radio areas like big conference rooms. The iPad only has one miserable dock connector or Lightning port, so it can either drive video OR be plugged in, but not both. The Surface RT looked like a great answer to this problem.Iâ€™m fair. If Iâ€™m going to complain about something, I want to have proof. I canâ€™t just say, â€œSurface RT suxxorzâ€? if I get frustrated. Rather than just return it and call it a day, I restored the device from scratch and tried the setup experience again. (Remember, Iâ€™m a former developer, so Iâ€™m used to trying to reproduce bugs.) I recorded videos of it in action to prove what was going on.But none of these mattered yesterday. Even with the restores, even with recording video of the problems, I got hammered. Hundreds of commenters on all kinds of sites said it was my fault.Last night, I went to bed with a plan. Iâ€™d drive down to the Microsoft store, buy another Surface RT, film the unboxing process, show how hard it is to find the behind-the-scenes desktop update panel on your own, and find out if it fixes the Skydrive and keyboard problems. (I already know the Mail updates donâ€™t fix the login/freeze problem, because Iâ€™d done those before filming the videos.)This morning, I woke up with a better plan. Iâ€™m moving on. I donâ€™t think thereâ€™s anything I could do to convince the hard-core fanboys out there that the Surface RT has problems â€“ because I realized that most of the commenters donâ€™t even own Surfaces. So many of the comments were flat out wrong, like saying thereâ€™s only one place for Surface updates and that Windows RT doesnâ€™t have a desktop mode. I think Iâ€™ve done a fair job of documenting the problems I ran into, and Iâ€™ve burned enough of my weekend time on it.And no, Iâ€™m not heading down to the Apple store to buy a new iPad, either. Iâ€™m still using a first-generation iPad 1, and believe me, itâ€™s just as flaky as the Surface RT is. Thereâ€™s no good presentation solution, the keyboards pale in comparison to the Surfaceâ€™s, and many apps are crashtastic.I donâ€™t have a single right answer for my gadget needs yet, but the fun part about being a geek in 2012 is that the options are nearly endless. The journey of finding the right gadget is just as much fun as the destination, and Iâ€™m looking forward to giving the next gadget a shot.Got a solution thatâ€™s available to buy today? Tell me in the comments.Itâ€™s not completely official yet, but it appears that Steven Sinofsky, Microsoftâ€™s President of Windows Division, agrees that the Word typing problem is a known issue and another update is forthcoming.Everybody who called me incompetent, please take your time in apologizing. Iâ€™m sure my blog would fall over immediately if all of you apologized at once.The real-world reviews are coming in, and theyâ€™re not good. Â Hereâ€™s a very long and detailed review from Chris Pirillo:The drones and other military aircraft have crowded the skies over the Horn of Africa so much that the risk of an aviation disaster has soared.Since January 2011, Air Force records show, five Predators armed with Hellfire missiles crashed after taking off from Lemonnier, including one drone that plummeted to the ground in a residential area of Djibouti City. No injuries were reported but four of the drones were destroyed.Predator drones in particular are more prone to mishaps than manned aircraft, Air Force statistics show. But the accidents rarely draw public attention because there are no pilots or passengers.As the pace of drone operations has intensified in Djibouti, Air Force mechanics have reported mysterious incidents in which the airborne robots went haywire.In March 2011, a Predator parked at the camp started its engine without any human direction, even though the ignition had been turned off and the fuel lines closed. Technicians concluded that a software bug had infected the â€œbrainsâ€? of the drone, but never pinpointed the problem.â€œAfter that whole starting-itself incident, we were fairly wary of the aircraft and watched it pretty closely,â€? an unnamed Air Force squadron commander testified to an investigative board, according to a transcript. â€œRight now, I still think the software is not good.â€?Djibouti is an impoverished former French colony with fewer than 1Â million people, scarce natural resources and miserably hot weather.But as far as the U.S. military is concerned, the country's strategic value is unparalleled. Sandwiched between East Africa and the Arabian Peninsula, Camp Lemonnier enables U.S. aircraft to reach hot spots such as Yemen or Somalia in minutes. Djiboutiâ€™s port also offers easy access to the Indian Ocean and the Red Sea.â€œThis is not an outpost in the middle of nowhere that is of marginal interest,â€? said Amanda J. Dory, the Pentagonâ€™s deputy assistant secretary for Africa. â€œThis is a very important location in terms of U.S. interests, in terms of freedom of navigation, when it comes to power projection.â€?The U.S. military pays $38Â million a year to lease Camp Lemonnier from the Djiboutian government. The base rolls across flat, sandy terrain on the edge of Djibouti City, a somnolent capital with eerily empty streets. During the day, many people stay indoors to avoid the heat and to chew khat, a mildly intoxicating plant that is popular in the region.Hemmed in by the sea and residential areas, Camp Lemonnierâ€™s primary shortcoming is that it has no space to expand. It is forced to share a single runway with Djiboutiâ€™s only international airport, as well as an adjoining French military base and the tiny Djiboutian armed forces.Apple has just announced a major executive shake-up: Senior VP of iOS software Scott Forstall is leaving Apple at the end of the year â€” he'll be serving in an advisory role to CEO Tim Cook until his departure. Additional executive changes include the departure of retail head John Browett, with Jony Ive, Bob Mansfield, Eddy Cue and Craig Federighi being tapped for additional responsibilities.To make up for the departure of Forstall, Jony Ive will now provide leadership and direction for human interface across the entire company â€” it sounds like Ive will be getting a major opportunity to bring his famed hardware design sensibility to Apple's software. Eddy Cue, who has been responsible for Apple's digital storefronts, will get increased responsibility in the form of Maps and Siri. Obviously, that's a major challenge for Cue to take on, and it isn't unreasonable to think that the failure of iOS 6 Maps at launch may have directly led to his removal as iOS VP.Craig Federighi, who previously served as VP of Mac software, will now be in charge of both iOS and OS X. Apple says this move will help unify software strategy across the two platforms; it sounds like he'll be the one most responsible for assuming Forstall's duties. Finally, VP Bob Mansfield â€” whose retirement was announced earlier this year before he announced his intentions to stay on in a less defined role â€” will head up a group known as Technologies, with a focus on semiconductor and wireless hardware.As for John Browett, Apple's Senior VP of retail is out after less than a year on the job. There's no word as to why he left (or was dismissed), but Apple says that a search for a replacement is underway. In the meantime, the company's retail team will report directly to Cook. All told, removing Browett and Forstall from Apple is a significant shake-up, as Forstall was a huge component behind the rapid rise and success of the iOS platform. Adam Lashinsky, author of Inside Apple, theorized on Twitter that Forstall was the "DRI" â€” directly responsible individual â€” for Maps and Siri, and thus "paid the price" for Apple's troubles with those two key iOS features. The DRI model was one that Steve Jobs believed strongly in during his role as Apple's CEO, and it looks like the concept lives on under Tim Cook's direction.CUPERTINO, Calif.--(BUSINESS WIRE)--AppleÂ® today announced executive management changes that will encourage even more collaboration between the Company's world-class hardware, software and services teams. As part of these changes, Jony Ive, Bob Mansfield, Eddy Cue and Craig Federighi will add more responsibilities to their roles. Apple also announced that Scott Forstall will be leaving Apple next year and will serve as an advisor to CEO Tim Cook in the interim. "We are in one of the most prolific periods of innovation and new products in Apple's history," said Tim Cook, Apple's CEO. "The amazing products that we've introduced in September and October, iPhone 5, iOS 6, iPad mini, iPad, iMac, MacBook Pro, iPod touch, iPod nano and many of our applications, could only have been created at Apple and are the direct result of our relentless focus on tightly integrating world-class hardware, software and services." Jony Ive will provide leadership and direction for Human Interface (HI) across the company in addition to his role as the leader of Industrial Design. His incredible design aesthetic has been the driving force behind the look and feel of Apple's products for more than a decade. Eddy Cue will take on the additional responsibility of SiriÂ® and Maps, placing all of our online services in one group. This organization has overseen major successes such as the iTunes StoreÂ®, the App Storeâ„ , the iBookstoreâ„  and iCloudÂ®. This group has an excellent track record of building and strengthening Apple's online services to meet and exceed the high expectations of our customers. Craig Federighi will lead both iOS and OS XÂ®. Apple has the most advanced mobile and desktop operating systems, and this move brings together the OS teams to make it even easier to deliver the best technology and user experience innovations to both platforms. Bob Mansfield will lead a new group, Technologies, which combines all of Apple's wireless teams across the company in one organization, fostering innovation in this area at an even higher level. This organization will also include the semiconductor teams, who have ambitious plans for the future. Additionally, John Browett is leaving Apple. A search for a new head of Retail is underway and in the interim, the Retail team will report directly to Tim Cook. Apple's Retail organization has an incredibly strong network of leaders at the store and regional level who will continue the excellent work that has been done over the past decade to revolutionize retailing with unique, innovative services for customers.Update: 9to5Mac has posted a team-wide email from Tim Cook thanking Forstall for his "many contributions to Apple over his career" and explaining that Mansfield will remain with the company for an additional two years. The full text is below.We are in one of the most prolific periods of innovation and new products in Appleâ€™s history. The amazing products that weâ€™ve introduced in September and October â€“ iPhone 5, iOS6, iPad mini, iPad, iMac, MacBook Pro, iPod touch, iPod nano and many of our applications â€“ could only have been created at Apple, and are the direct result of our relentless focus on tightly integrating world-class hardware, software and services. Today, I am announcing changes that will encourage even more collaboration between our world-class hardware, software and services teams at all levels of our company. As part of these changes, Jony Ive, Bob Mansfield, Eddy Cue, and Craig Federighi will be taking on more responsibilities. I am also announcing that Scott Forstall will be leaving Apple next year and will serve as an advisor to me during the interim. I want to thank Scott for all of his many contributions to Apple over his career. Jony Ive will provide leadership and direction for Human Interface (HI) across the company in addition to his longtime role as the leader of Industrial Design. Jony has an incredible design aesthetic and has been the driving force behind the look and feel of our products for more than a decade. The face of many of our products is our software and the extension of Jonyâ€™s skills into this area will widen the gap between Apple and our competition. Eddy Cue will take on the additional responsibility of Siri and Maps. This places all of our online services in one group. Eddy and his organization have overseen major successes such as the iTunes Store, the App Store, the iBookstore and iCloud. They have an excellent track record of building and strengthening our online services to meet and exceed the high expectations of our customers. Craig Federighi will lead both iOS and OS X. We have the most advanced mobile and desktop operating systems on the planet, and bringing together our OS teams will make it even easier to deliver our best technology and user experience innovations to both platforms. Craig recently led the very successful release of Mountain Lion. Bob Mansfield will lead a new group, Technologies, which combines all of our wireless teams across the company in one organization, allowing us to innovate in this area at an even higher level. This organization will also include all of our semiconductor teams, who have some very ambitious plans. As part of this, I am thrilled to tell you that Bob will remain with Apple for an additional two years. Bob has led some of our most challenging engineering projects for many years. Additionally, John Browett is leaving Apple. Our search for a new head of Retail is already underway. In the meantime, the Retail team will report directly to me. Retail has an incredibly strong network of leaders at the store and regional level, and they will continue the excellent work theyâ€™ve done over the past decade to revolutionize retailing with unique, innovative services and a focus on the customer that is second to none. This phenomenal team of talented and dedicated people works their hearts out making our customers happy. They have our respect, our admiration and our undying support. Please join me in congratulating everyone on their new roles. Iâ€™d like to thank everyone for working so hard so that Apple can continue to make the worldâ€™s best products and delight our customers. I continue to believe that Apple has the most talented and most innovative people on the planet, and I feel privileged and inspired to be able to work with all of you.Update: In response to the impact of Hurricane Sandy, Comcast is opening its XFINITY WiFi hotspots to non-Comcast subscribers in PA, NJ, DE, MD, DC, VA, WV, MA, NH and ME until Nov. 7. Users should search for the network "xfinitywifi" and click on "Not a Comcast subscriber?" at the bottom of the sign-in page. Users should select the "Complimentary Trial Session" option from the drop down list. The Open Wireless Movement thanks Comcast for helping out!In troubled times, it's important to help each other out. Right now, we're witnessing an unprecedented hurricane hitting the Eastern Seaboard of the United States, and the ensuing damage and power outages are crippling rescue efforts, businesses large and small, and personal communications.Communication is critical in time of crisis, and the Internet allows for the most effective way of getting information in and out. With readily available networks, governmentÂ officials could use tools like Twitter to quickly spread information, citizen reports could help focus assistance where it is needed most, and social media updates could help reassure friends and loved onesâ€”keeping mobile phone lines open for emergencies.To take advantage of the Internet, people should not have to attempt to skirt restrictive Terms of Service to attempt to tether their smartphones. And tethering would not be necessary if there were ubiquitous open wireless, so that anyone with a connection and power can share their networkÂ with the neigborhood.Last year, we wrote a post titled "Why We Need An Open Wireless Movement." Today,Â EFF is proud to announce the launch of the Open Wireless Movementâ€”located at openwireless.orgâ€”a coalition effort put forth in conjunction with nine other organizations: Fight for the Future, Free Press, Internet Archive, NYCwireless, the Open Garden Foundation, OpenITP, the Open Spectrum Alliance, the Open Technology Institute, and the Personal Telco Project.Aimed at residences, businesses, Internet service providers (ISPs), and developers, the Open Wireless Movement helps foster a world where the dozens of wireless networks that criss-cross any urban area are now open for us and our devices to use.The Open Wireless Movement envisions a world where people readily have access to open wireless Internet connectionsâ€”a world where sharing one's network in a way that ensures security yet preserves quality is the norm. Much of this vision is attainable now. In fact, many people have routers that already feature "guest networking" capabilities. To make this even easier, we are working with a coalition of volunteer engineers to build technologies that would make it simple for Internet subscribers to portion off their wireless networks for guests and the public while maintaining security, protecting privacy, and preserving quality of access. And we're working with advocates to help change the way people and businesses think about Internet service.We're also teaching the world about the many benefits of open wireless in order to help society move away from closed networks and to a world in which open access is the default. We are working to debunk myths (and confront truths) about open wireless while creating technologies and legal precedent to ensure it is safe, private, and legal to open your network.We believe there are many benefits to having a world of open wireless. Two of the big ones for us have to do with privacy and innovation.Open wireless protects privacy. By using multiple IP addresses as one shifts from wireless network to wireless network, you can make it more difficult for advertisers and marketing companies to track you without cookies. Activists can better protect their anonymous communication by using open wireless (though TorÂ is still recommended).Innovations would also thrive: Smarter tablets, watches, clothing, carsâ€”the possibilities are endless.Â In a future with ubiquitous open Internet, smartphones can take advantage of persistent, higher quality connections to run apps more efficiently without reporting your whereabouts or communications. Inventors and creators would not have to ask permission of cell phone companies to utilize their networks, both freeing up radio spectrum and reducing unnecessary barriers to entry.This movement is just beginning, but in a sense it has always been around. People, businesses, and communities have already been opening up their wireless networks, sharing with their neighbors, and providing an important public good. We want this movement to grow without unnecessary legal fears or technical restraints.Join the Open Wireless Movement. Whether you're a household or small business, a technologist or a student, we need your support. Check out openwireless.org for more information, and spread the word.A backlash among Reddit users has seen BitTorrent Inc. criticized over the way revenue-generating addons were presented in parallel with uTorrent client downloads. The company informs TorrentFreak that it always considers feedback, aims to provide a good customer experience, and will introduce changes soon. But whatever they are, is it really possible to please all of the people all of the time, especially ones whose requirements are â€œno-strings freeâ€? at all times?Apparently everything is available for free on the web these days. Music, movies, TV shows, games â€“ you name it â€“ itâ€™s all just a click away.New business models must be found, the tide is way too strong to hold back, the genie is out of the bottle. Itâ€™s reportedly get real or get out time, or so the sound bites go.BitTorrent users, in one form or another, have been held to blame for much of the above scenario, along with their main weapon of choice, uTorrent. But market forces are interesting beasts and ones that donâ€™t exist in a vacuum.BitTorrent Inc., the company behind the completely legal uTorrent, has worked hard to develop both itself and its software, but as it grows so do its costs. Somehow revenue has to be generated and these days, when youâ€™re a company employing around 80 staff, that has to be a significant amount.So, just like the entertainment companies who struggle to make money against free, BitTorrent Inc. has to employ techniques to give away their free product, in this instance uTorrent, and bring in the bucks at the same time.In part this is achieved by selling uTorrent Plus, which is essentially the regular uTorrent with anti-virus, media playing and conversion functions built in. Revenue is also generated by bundling optional addons, such as a toolbar, with the free uTorrent, but a new method of offering these extras has managed to irritate a bunch of Reddit users.The complaint centers around a single but very important button on the uTorrent site â€“ the â€œFree Downloadâ€? button, as illustrated below.The problem is that the button isnâ€™t a simple one-click download. Once a user hovers over it ready to click, the button and surrounding areas quickly change to include extra information.The very eager user will simply see â€œDownloadâ€? directly under their mouse pointer and will just click away, but the more cautious will notice that there are three options â€“ all preselected â€“ which relate to extra features and bundled revenue-generating software. You can test for yourself here.Obviously BitTorrent Inc. need to make money, but the main complaints seem to center around the way this download page has been configured to encourage a skipping over the details (and therefore the installation of potentially unwanted software) in order to obtain a quick download.While critics might argue that people should read what theyâ€™re agreeing to before clicking, BitTorrent Inc. say that the changes are recent and were implemented to streamline the uTorrent installation experience.â€œWeâ€™ve been offering the toolbar for years as a way to support the development of our software so users can get it free,â€? a spokesman told TorrentFreak.â€œWe recently moved the toolbar to the download page so we could have more flexibility in how we describe the toolbarâ€™s torrent-specific features and also shorten and clean up our installer, an ongoing process.â€?But what is clear from the posts on Reddit and elsewhere is that the changes arenâ€™t popular. With that in mind, BitTorrent Inc., which has a record of listening to its users, says it will do some restructuring.â€œWe have read feedback including the Reddit posts, and are planning adjustments to improve the experience,â€? their spokesperson concludes.While we wait for the changes to be confirmed (they will apparently arrive tomorrow), what is interesting to observe is how relatively easily some BitTorrent users, despite getting a free product in return, are upset by tactics they perceive as being less than upfront.The Reddit thread is full of threats to switch to different clients and as always there is a tendency to suggest clients with less intrusive revenue generating mechanisms, fewer adverts, then ultimately ones that offer a plain client with nothing added at all.It seems that having to compete with free is a reality even for BitTorrent clients these days. How times change.We first posted about the situation at our data center 8:57 am on Tuesday. 60 hours later, it has stabilized. What does that mean? Mostly, it means that the methods being used to power our services are unremarkable. Data centers throughout New York City and the surrounding area are using the same types of generators to keep countless hosted services running.It also means that we will spare you the hourly status reports on the nuts and bolts of maintaining power at our data center. Barring a completely new problem, the only further post here will be to let you know that our data center is back on the grid.The total actual downtime during this incident was approximately three hours, from about 10:45am to about 2pm on Tuesday. This was self-imposed, to protect our customers against data corruption. If this has materially impacted your ability to do business, please let us know.As is our policy with any unplanned downtime, we are planning a full postmortem, which will appear on this site. Though you will see no new evidence of it, the entire Fog Creek team continues to work full steam on contingency plans. If something unforeseen happens in the near future, or when the next natural disaster strikes, we will be able to respond quickly and effectively.The Bulgarian blogger and digital rights activist who made headlinesÂ on Tuesday when he reported acquiring more than one million Facebook data entries for just $5, said Friday he is cooperating with Facebook as it conducts an internal investigation, but won't comply with the company's request to remove blog posts or not talk about the investigation.In an interview with ReadWrite, Bogomil Shopov said he had been contacted by Facebook's Platform Policy Team after revealing on his blog that he had acquired the list, which included email addresses of active Facebook users who were primarily located in the U.S., Canada and Europe. Shopov said officials with the company were upset because they feared his public revelation would upend an internal investigation.(Read Shopov's new blog post: Mixed Feelings After Conversation With Facebook.)Â Facebook declined elaborate on the details of its investigation.â€œFacebook is vigilant about protecting our users from those who would try to expose any form of user information. In this case, it appears someone has attempted to scrape information from our site," Facebook spokesman Chris Kraeuter said in an email statement. "We have dedicated security engineers and teams that look into and take aggressive action on reports just like these. We continue to investigate this specific individual.â€?Â In addition to requesting that he keep conversations with Facebook private, the company also requested that Shopov destroy the data after sending a copy to Facebook. Shopov said he complied with the request to destroy the data but was continuing to speak with news outlets to make Facebook users aware of the breach.That didnâ€™t sit well with Facebook, according to Shopov.Â â€œTheir version is [they are conducting] an â€˜internal investigationâ€™ and one of the reasons they are angry about my blog posts is that the seller can â€˜go deepâ€™,â€? Shopov said, explaining Facebook is concerned the seller will disappear before the investigation can figure out how the data was obtained.Shopov provided ReadWrite with a cached link to the site where he purchased the data. The offer was removed within two days after his initial blog postÂ on Tuesday, October 23, but the cached version shows that the seller obtained the data through an unidentified, third-party application. This raises the question of whether there's an international black market where anyone can buy supposedly secret Facebook user data.Â Shopov verified that some of the addresses were legitimate and had planned to notify people on the list that he had purchased the data. Facebook asked him to not notify people included on the list, Shopov said.â€œWe agreed with Facebook not to do that,â€? he said. â€œThat was actually my first reaction, to tell them and to teach them about their rights.â€?Who says that Android tablets arenâ€™t cool? Research firm Strategy Analytics says that shipments of Android tablets surged to a new high in the third quarter of 2012, accounting for 41% of all tablets shipped.Â Neil Mawston, Strategy Analyticsâ€™ executive director, says that thereâ€™s no one Android tablet responsible for the surge, which is more due to a large influx of devices from a wide variety of vendors including â€œASUS (2357), Samsung (005930) and Nook.â€? Shipments of Appleâ€™s (AAPL) iPad lineup, meanwhile, shrank to 57% of the market as â€œdemand for tablets slowed due to ongoing economic uncertainty and consumers holding off purchases in anticipation of multiple new models, like the iPad Mini, during the upcoming Q4 holiday season.â€? Strategy Analyticsâ€™ full press release is posted below.BOSTONâ€“(BUSINESS WIRE)â€“According to the latest research from Strategy Analytics, global tablet shipments reached 25 million units in the third quarter of 2012. Apple iOS slipped to 57 percent global market share, allowing Android to capture a record 41 percent share.Peter King, Director at Strategy Analytics, said, â€œGlobal tablet shipments reached 24.7 million units in Q3 2012, rising a sluggish 43 percent from 17.2 million in Q3 2011. Demand for tablets slowed due to ongoing economic uncertainty and consumers holding off purchases in anticipation of multiple new models, like the iPad Mini, during the upcoming Q4 holiday season. Apple shipped a disappointing 14.0 million iPads worldwide and captured 57 percent share in the third quarter of 2012, dipping from 64 percent a year ago. Appleâ€™s slowdown allowed the Android community to make gains and Androidâ€™s global share of the tablet market now stands at a record 41 percent.â€?Neil Mawston, Executive Director at Strategy Analytics, added, â€œAndroid captured a record 41 percent share of global tablet shipments in Q3 2012, jumping from 29 percent a year earlier. Global Android tablet shipments doubled annually to 10.2 million units. No single Android vendor comes close to Apple in volume terms at the moment, but the collective weight of dozens of hardware partners, such as Asus, Samsung and Nook, is helping Googleâ€™s Android platform to register a growing presence in tablets.â€?Other findings from the research include:* Global tablet shipments grew just 43 percent annually in Q3 2012, compared with 289 percent annually in Q2 2011. This was the weakest growth rate since the modern tablet industry began in Q2 2010;* Microsoft captured a niche 2 percent global tablet share in Q3 2012. The imminent release of the new Windows 8 operating system will likely drive Microsoft tablet volumes higher during the Q4 2012 holiday season.The full report, Global Tablet OS Market Share: Q3 2012, is published by the Strategy Analytics Tablet & Touchscreen (TTS) service, details of which can be found here: http://tinyurl.com/bpqpnbs.Last night, a transformer exploded at a Con Edison plant in lower Manhattan, sparking a flurry of tweets, texts and Facebook posts from residents who witnessed or caught the event on camera. Power failed from 39th Street all the way to the southern tip of Manhattan, and the affected area likely will not regain power for up to a week. So far, authorities donâ€™t know whether the explosion was directly related to the storm since it happened just as Con EdÂ intentionallyÂ cut power to 65,000 customers in an effort to protect equipment, CBS News writes.Although we donâ€™t yet know what happened at this particular plant, we do know several general problems that can cause transformers to explode. Popular Mechanics explains:When flooded with too much electricity, the sudden surge can cause a transformer explosion. As transformers detect an energy spike, theyâ€™re programmed to turn off, but it can take up to 60 milliseconds for the shutdown. However fast those milliseconds may seem, they still may be too slow to stop the electrical overload. A chamber full of several gallons of mineral oil keeps the circuits cool, but given too much electricity, the circuits fry and melt, failing in a shower of sparks and setting the mineral oil aflame. Mineral oil, in turn, combusts explosively and rockets transformer scything into the air. All it takes is a trigger, a corroded or faulty wire, and the circuits surge will get ahead of the breaker.Salt from sea water, for example, can create hazardousÂ conditionsÂ for underground electrical systems since it acts as a corrosive agent. Old transformers can explode when their insulating materials begin to fail, too.We should have a more specific answer about what happened during Hurricane Sandy to trigger the transformer explosion soon, but hopefully the thousands without electricity will have their power restored even sooner.An Unholy Alliance of Unusual Weather and Scarce Coal Nuked Indiaâ€™s Power GridÂ  How Smart Can a City Get?Â Since ACTA was decisively beaten on 4th July 2012, the first time a free trade agreement had been scuppered by the people of EU member nations, the big business lobbyists have taken heed and resolved to change in order to be more successful. Hence the secrecy. CETA and the EU-India trade agreement are the next big battles. We need your help.The term â€œFree Trade Agreementâ€? is a misnomer. The idea is to remove barriers, taxes, and tariffs, but since people can end up being shackled to a multinational corporationâ€™s agenda, the only freedom is in the ability of the corporations to operate in ways that often end up utterly destroying local economies or harnessing law enforcement agencies to protect their interests. The worst part is that we the taxpayers have to foot the bill for our losses of national sovereignty and civil rights. We saw ACTA off in July, but there are two more major agreements to deal with and we need to be ready to contact our M.E.P.s when the time comes.CETA is the Canada-Europe Treaty Agreement. Itâ€™s so bad, Canadian cities and local authorities want to be able to opt out of it. The issues theyâ€™re having centre on the onerous procurement rules that would favor European corporations over local suppliers but there are implications for the internet, too, in the form of the ACTA-style intellectual property chapter, which Dr. Michael Geist published on his blog. Itâ€™s only an old leaked draft, but getting hold of the actual documents has been an exercise in frustration. However, it seems that Bilaterals.org has been able to preserve a copy of the Draft Consolidated Text. Despite the lack of information available, tech blogs such as Techdirt and Computerworld are picking up the story.The European Union has been secretly negotiating a free trade agreement with India since 2007 that is worryingly similar to ACTA. Intellectual property rights enforcement would include border detention and seizure measures of goods being imported by India, exported by India or in transit via Indiaâ€™s ports or airports. This could affect the generic drugs that India produces for its people. Needless to say, intellectual property rights are on the menu, mostly for pharmaceuticals, it has to be said, but since we have no access to the documents involved itâ€™s fair to say itâ€™s likely to include internet provisions, too. David Martin MEP, rapporteur for the European Unionâ€™s International Trade Committee, whose recommendations helped to pull ACTA down in July, is joining unions and international NGOs to oppose the treaty and the secrecy that goes with it. Indian business groups agree, fearing that European imports will jeopardize local production.It is essential that we mobilize opposition to these free trade agreements, not just because they are unjust, but because, if they are ratified, they will bring back the spectre of ACTA, just as E.U. Trade Commissioner Karel De Gucht assured us back in July.Microsoft has just announced that developers at its Build 2012 conference will receive 100GB of SkyDrive storage, and a free 32GB Surface RT. Speaking enthusiastically about the developer opportunity ahead, Microsoft CEO Steve Ballmer guaranteed developers in the crowd that "this will be the best opportunity software developers will see.""Hundreds of millions of people are just aching to use your applications," said Ballmer, before announcing the giveaway for Build attendees. The crowd was understandably excited, and Ballmer promised developers that Microsoft would do more marketing and "better marketing" for Windows 8.Update: Nokia's Richard Kerris joined Microsoft on stage at Build today and also announced a free Lumia 920 for attendees.Last week Glassdoor published its most recent software engineering salary report. Short version: it pays to code. Google and Facebook employees earn a base salary of ~$125K, not counting benefits, 401k matching, stock options/grants, etc., and even Yahoo! developers pull in six figures. Everyone knows why: ask anyone in the Valley, or NYC, or, well, practically anywhere, and theyâ€™ll tell you that good engineers are awfully hard to find. Demand has skyrocketed, supply has stagnated, prices have risen. Basic economics.But why has the supply of good engineers remained so strained? Weâ€™re talking about work that can, in principle, be performed by anyone anywhere with a half-decent computer and a decent Internet connection. Development tools have never been more accessible than in this era of $100 Android phones, free-tier web services, and industry-standard open-source platforms. Distributed companies with employees scattered all around the world are increasingly normal and acceptable. (I work for one. Weâ€™re hiring.) And everyone knows that software experts make big bucks, because software is eating the world. Whatâ€™s more, technology may well be destroying jobs faster than it creates them. Basic economics would seem to dictate that an exponentially larger number of people will flood into the field, bringing salaries back down to earth despite the ever-increasing demand.But reality has stubbornly refused to follow that dictation. Even way back during the first dot-com boom people were already predicting that American and European coders would soon be driven into the poorhouse by a flood of competition from low-cost nations like India and Brazil. But thereâ€™s still no sign of that happening. Why not? And when will it happen, if ever?Well. I have a theory. Iâ€™ve spend the last couple of days chilling out in Chiang Mai, northern Thailand, a city where you could live like royalty and save money while making merely half of Googleâ€™s average developer salary. Which doesnâ€™t tempt me â€“ I prefer Where Things Happen to Away From It All â€“ but has tempted thousands of expats who now live here. And their presence has sparked a possible explanation for this apparent paradox.To be clear, Iâ€™m only talking about very-good-to-excellent developers. Everyone claims to only hire â€œA-listers,â€? and that may even be true of a select few companies, including Facebook and Google. (Though even B-listers and C-listers are in relative demand.) Think of such skilled engineers as emerging from the end of a pipeline which draws from the entire population of the world. Economic incentives act like gravity, pulling almost everyone down that pipe â€“ so what are the stages that filter people out of it nonetheless?First, you have to grow up wealthy enough to have a decent education, some exposure to technology, and the ability to choose between options in your life, which immediately rules out most of the planet. Then you have to have both an interest in and a talent for development, and thereâ€™s evidence that that talent is rare: â€œbetween 30% and 60% of every university computer science departmentâ€™s intake fail the first programming course.â€œ. Then you either have to get a good professional education â€“ eg at a good university like Indiaâ€™s IIT campuses â€“ or supplement a crappy one with home hacking or on-the-job training.(Or maybe, maybe, learn-coding-at-home sites like Codecademy and the likeâ€“but Iâ€™m pretty skeptical about those. Iâ€™ve said before that I think think such services are like learning French from books, and then going to France and finding out that you canâ€™t actually communicate and it would take you years to be become fluent. Programming is like English: itâ€™s fairly easy to learn the rudimentary basics, but very hard to master.)Regardless, all of those filters should be allowing many more people through every year. The world as a whole is much wealthier than it was twelve years ago. (Thatâ€™s when I was last in Thailand. This time around itâ€™s a different and far more prosperous place.) A fixed proportion of people may have the programming gene â€” though Iâ€™ll be watching Estoniaâ€™s experiments with interest â€” but thereâ€™s little doubt that interest has erupted. Top-notch university courses are available online worldwide, and industry-standard development tools are within reach of all.But itâ€™s the very last stage that matters most. Even after youâ€™ve gotten your basic programming education, you still have to put in your thousands of hours to achieve mastery. That doesnâ€™t mean doing the same thing again and again for thousands of hours; it means challenging yourself with new tools, new languages, new objectives. Otherwise you get people writing code of the sort I see all too often these days, when HappyFunCorp (my employer) is brought on to clean up someone elseâ€™s hot mess:My theory that if itâ€™s sheer economics, the lure of a better paycheck, that initially draws you into software engineering, then youâ€™re much less likely to master it. Instead youâ€™ll advance to the point at which youâ€™re reasonably happy with your paycheck, which studies indicate is about $70,000/year in America. (But much less in Chiang Mai or Bangalore.) So my theory is that there are many more software engineers out there â€” but the ones drawn in by economic forces are content to compete with each other for mediocre (but happy-making) jobs, rather than put in the thousands of hours of mentally gruelling work required to become really good at what they do.(Donâ€™t get me wrong: that work is fun, too. But undeniably gruelling.)So why arenâ€™t there more people drawn into the field out of sheer interest? Because when youâ€™re poor, which most of the world is, money is more important than passion. Itâ€™s not until you reach a near First-World level of development that pursuing your passions rather than escaping poverty seems like a reasonable and/or admirable thing to do. So if my theory is correct, the shortage of excellent engineers will eventually alleviate or even end, as the world grows wealthier everywhere â€¦ but not for another decade or more.When you have a question, finding the answer should be effortlessâ€”wherever you are and whatever device youâ€™re using. The new Google Search app for iPhone and iPad helps you to do just that with enhanced voice search that answers any question with the comprehensive Google search results you know and love.Fast and accurate voice recognition technology enables Google to understand exactly what youâ€™re saying. Getting an answer is as simple as tapping on the microphone icon and asking a question like, â€œIs United Airlines flight 318 on time?â€? Your words appear as you speak, you get your answer immediately andâ€”if itâ€™s short and quick, like the status and departure time of your flightâ€”Google tells you the answer aloud.You can get answers to an increasingly wide variety of questions thanks to Knowledge Graph, which gives our search technology an understanding of people, places and things in the real world. Here are a few of the questions that Google can answer:â€œWhat does Yankee Stadium look like?â€? Google will show you hundreds of pictures instantly.â€œPlay me a trailer of the upcoming James Bond movie.â€? The trailer starts playing immediately right within Google Search. â€œWhen does daylight savings time end?â€? The answer will appear above the search results, so you can set your clock without having to click on a link.  â€œWhoâ€™s in the cast of The Office?â€? See a complete cast list and find out who made you crack up last night. Hue is a series of light-emitting-diode bulbs controlled from a handheld Apple device through a household Wi-Fi network. At the Apple end, users can control the lights using a free app on their iPhone, iPod or iPad.The bulbs offer a variable white light, mimic incandescent lights and will produce more than 16 million colours. While LED lighting has been praised for its extreme power-saving attributes, the harsh whiteness of the light has taken longer for technology to control."I was able to change the colours of the light bulbs in different rooms, adjust the brightness level or turn the lights off and on with one touch from my iPad," wrote Mashable reviewer Andrea Smith, who tested the system for several days.As well, the system can memorize lighting combinations for people to reuse in the future and can operate on a timed on-off basis. Lighting combinations and programs can be shared through social media."I pressed a button on the bridge which immediately identifies all three lights," Smith wrote."Using the app on my iPad, I was able to rename the lights, calling them living room, family room and office. I had fun sliding the bar from left to right, which changes the intensity of the bulb's colour; it was like having a dimmer switch built into my mobile device."The system offers flexibility and control that was once limited to lighting systems worth thousands of dollars in commercial applications.For all that, however, it isn't cheap. The introductory kit â€” three bulbs and a ZigBee bridge that attaches to a Wi-Fi router â€” costs $199 in Canada. Additional bulbs cost $59 each. As many as 50 bulbs can be operated on one system.In terms of power, the bulbs are rated at up to 8.5 watts, and each produces light of 600 lumens â€” roughly equivalent to a 50-watt incandescent bulb.Dear Lord: Please make my words sweet and tender, for tomorrow I may have to eat them.Connection woes hit popular services across the Internet. So far, though, it's unclear if they are related.A mysterious rash of outages struck the Internet today, crippling major services for hours at a time. It isn't clear whether they're related.Google Apps Engine. Google said that at about 7:30 a.m., an unnamed component of App Engine "began experiencing slow performance and dropped connections." Users began seeing slow response times and had trouble connecting to services. At the moment, most App Engine users and services are being affected. "Google engineering teams are investigating a number of options for restoring service as quickly as possible, and we will provide another update as information changes, or within 60 minutes," Google's Max Ross said.Tumblr. Around the same time Google Apps Engine began having problems, Tumblr tweeted it was having problems of its own.Dropbox. The Next Web and other sites also reported having issues with Dropbox, though the service was working fine when we checked.Meanwhile, there's evidence that the outages have affected the wider Web. The Internet Traffic Report showed a sharp decline in traffic today:It also showed an increase in packet loss, which is a measurement of a connection's reliability.Update, 1:10 p.m. Tumblr and Google App Engine have been restored.Napier & Son was the most successful British manufacturer of aircraft engines in the 1920s and 30s with their 12-cylinder Napier Lion powering 163 different types of aircraft between 1918 and 1935. Over that 17 year period the Lion grew from 450 to 1350 horsepower and was, for awhile, the most powerful aircraft, boat, and car engine in the world, holding world speed records in all three venues at the same time. And then the Napier Lion was suddenly gone â€” a lesson from which Microsoftâ€™s Steve Ballmer could benefit if he and his company donâ€™t repeat it.Napier perfected their Lion engine over those 17 years, improving it in every way until it was the best and most efficient engine of its class in the world. Then, seemingly overnight, the class changed as air forces and record setters alike suddenly needed more than the 1,350 horsepower a finely-tuned Lion could deliver. Napierâ€™s Lion gave way to Rolls-Royceâ€™s larger and innately more powerful Merlin and Griffon engines and Napier, for all intents and purposes, was gone.Napier milked its technical and market advantages for a little too long.What does this have to do with Steve Ballmer and Microsoft? They are Napier, circa 1935 and their Lion is called Windows.Windows 8 shipped last week to mixed reviews. Ballmer himself called it â€œa bold re-imaginingâ€? of Windows. Itâ€™s bold alright, but not bold enough. Windows is doomed.We can argue all day about whether Windows 8 is better or worse than Windows 7 or even Windows 9, but the real issue here isnâ€™t the software at all but the platform, by which I mean the desktop PC. Companies, governments, families, schools, and individuals are all buying fewer desktop PCs than they used to. Desktop growth has reversed and international desktop expansion is slowing as even that market matures. This year will probably mark Microsoftâ€™s highest desktop sales ever in dollar volume, which sounds good, except that next year sales will be less as they will again the year after and every year past that.Six years from now (four hardware generations) Windows will be dead. Or free.And for all his bold re-imagining in New York last week, Steve Ballmer knows this, and thatâ€™s his dilemma.Desktops are fading now, notebooks will be fading soon, both to be replaced by tablets and smart phones where Microsoft not only doesnâ€™t dominate, they arenâ€™t even among the major players.Death of the desktop is clear not because Windows desktop sales are declining but because Macintosh desktop sales are declining. When Mercedes (Apple) begins to suffer declining unit sales, what does it mean for GM (Microsoft)? Not good.The only option is to invent the future, which Ballmer and Microsoft are attempting to do by entering the tablet hardware business (again emulating Apple) and cutting bold smart phone deals with outfits like Nokia. But Microsoft, for all its posturing and $1 billion marketing budgets, isnâ€™t any good at inventing the future and knows it. Ballmer lacks confidence that Redmond can invent itâ€™s way out of the current hole. And because he lacks confidence, as does nearly everyone else at Microsoft, of course it wonâ€™t happen.Microsoft didnâ€™t invent the PC but benefited from its invention. Microsoft didnâ€™t invent BASIC, they didnâ€™t invent the PC operating system, they didnâ€™t invent word processor, spreadsheet, or presentation applications, they didnâ€™t invent PC games, they didnâ€™t invent the graphical user interface, they didnâ€™t invent the notebook or the tablet, they didnâ€™t invent the Internet, they didnâ€™t invent the music player or the video game, but they benefited from all these things.Like Blanche DuBois, Microsoft has relied on the kindness of strangers.Microsoft may have invented the smart phone. More on that below.Having not invented any of the products it is known for, why should we expect Microsoft to invent its way out of declining markets? We shouldnâ€™t.Even video games are in decline and we now see Microsoft trying to turn its 30 million-strong xBox installed base into something like a cable TV network in order to milk that franchise beyond what would otherwise be its death.Ballmer knows all this. And like Napier, he can keep building his old product line with a twist or two until the market drops out from under him or he can do some real re-imagining and turn Microsoft into a completely different company.I donâ€™t think he will do it, though, because I donâ€™t think he can do it. Even if Steve Ballmer could envision a better future for Microsoft built on true technical leadership, I donâ€™t think he or his company could follow-through. They are just making too much money doing the old stuff to truly embrace anything new.Until itâ€™s too late.This does not mean Microsoft is going away. Their smart phone patents score them $15 for every new Android license of which there are 1.3 million every day. Thatâ€™s $20 million per day ($7.3 billionÂ per year) to Microsoft for doing, well, nothing.What Steve Ballmer and Microsoft need to do is clean up their act, quietly trim expenses, maybe even sell a few product lines, and start to seriously stash away cash toward the post-Windows, post-Office world of 2018.Yes, post-Office. What else can be meant by bundling Office with Windows RT than its value is headed to zero?If Microsoft can continue to pretend it is big while actually becoming small, they might end up in 2018 with a small residual product line sitting atop $100 billion in cash. Then Ballmer can hand that money to Warren Buffett or to Buffettâ€™s successor and let them manage Microsoft as a mutual fund rather than a technology company.This is the only future I see for Microsoft because I think Steve Ballmer is a rational man, he understands this, and he sees himself as the only plausible steward for such a sneaky transition. Otherwise, simply as a huge Microsoft shareholder he would have long ago fired himself.I think this is exactly what has been happening at Microsoft for at least the last 2-3 years, ever since the plan to buy Yahoo cratered.Ballmer isnâ€™t stupid and he isnâ€™t deluded, heâ€™s a man with a plan â€” a plan weâ€™re just not supposed to know about yet.Nothing else makes sense to me.This entry was posted on Sunday, October 28th, 2012 at 12:15 am and is filed under 2012, Business, Companies, Computing, Internet, Predictions, Software, Tablets, Technology. You can follow any responses to this entry through the RSS 2.0 feed. You can leave a response, or trackback from your own site.Sitting U.S. President Ford was visiting San Francisco in 1975 when a woman attempted to shoot him. A former marine named Oliver Sipple grabbed the gun, preventing the assassination attempt. When the press began contacting him, he asked that his sexuality not be discussed. While Sipple was very active in the gay menâ€™s scene in the Castro, he was not out to family or work. But Harvey Milk, a famous gay rights activist, chose to out him so the public could see that gay men could be heroes, too.The cost to Sipple was devastating. The White House distanced itself from him, his family rejected him, and he sunk into a dark depression. He gained massive amounts of weight, began drinking profusely, and died at the ripe young age of 47. Many around Sipple reported that he regretted his act of heroism and the attention resulting from it. But for Harvey Milk, the potential social good from using Sippleâ€™s story far outweighed what he perceived as the costs of outing him.This is a hard moral conundrum, in part because Sipple was clearly a â€œgoodâ€? guy who had done a good deed. But what if he wasnâ€™t? What are the moral and ethical costs of outing people and focusing unwanted attention on them?Two weeks ago, Gawker journalist Adrian Chen decided to unmask the infamous Reddit troll â€œViolentacrezâ€? as Michael Brutsch. When Chen contacted him, Brutsch did not attempt to deny the things he had done. He simply begged Chen not to publish his name, citing the costs that publicity would have on his disabled wife. Chen chose to publish the piece â€“ including Brutschâ€™s pleas and promises to do anything that Chen asked in return for not ruining his life. As expected, Brutsch lost his job and the health insurance that paid for his wifeâ€™s care; Chen reported this outcome three days later. Many celebrated this public shaming, ecstatic to see a notorious troll grovel.Although none of his actions appeared to be illegal, itâ€™s hard to call Brutsch a â€œgoodâ€? guy. He had created settings where people could share deeply disturbing content. He enticed people to reveal their ugliest sides. In many ways, Brutsch was a classic troll, abusing technology and manipulating the boundaries of free speech to provoke systematic prejudices and harassment for his own entertainment. He got joy from making others miserable.There are many different reasons to unmask people, out them, or make them much more visible than they previously were. Sometimes, the goal is to celebrate someoneâ€™s goodness. At other times, people are made visible to use them as an example â€¦ or to set an example. People are outed to reveal hypocrisy and their practices are made visible to shame them.In identifying Butsch and shining a spotlight on his insidious practices, Chenâ€™s article condemns Butschâ€™s choice of using the mask of pseudonymity to hide behind actions that have societal consequences. Public shaming is one way in which social norms are regulated. Another is censorship, as evidenced by the Reddit communityâ€™s response to Gawker.Yet, how do we as a society weigh the moral costs of shining a spotlight on someone, however â€œbadâ€? their actions are? What happens when, as a result of social media, vigilantism takes on a new form?Â How do we guarantee justice and punishment that fits the crime when we can use visibility as a tool for massive public shaming? Is it always a good idea to regulate what different arbiters consider bad behavior through increasing someoneâ€™s notoriety â€“ or censoring their links?As the Gawker/Reddit story was unfolding, another seemingly disconnected case was playing out. In a town outside of Vancouver, a young woman named Amanda Todd committed suicide a few weeks after posting a harrowing YouTube video describing an anonymous stalker she felt ruined her life. The amorphous hacktivist collective known as â€œAnonymousâ€? decided to make a spectacle of the situation by publishing personally identifiable information on â€“ â€œdoxxingâ€? â€“Â Toddâ€™s stalker. They identified a 32-year-old man, enabling outraged people to harass him. Yet it appears they got the wrong person. Earlier this week, Canadian police reported that Toddâ€™s stalker was someone else: reportedly a 19-year-old.Needless to say, this shift in information doesnâ€™t relieve the original target of the public shame he felt from Anonymousâ€™ pointed finger. It doesnâ€™t wipe his digital record clean. He has to deal with being outed â€“ in this case, wrongly â€“ going forward.By enabling the rapid flow of information, technology offers us a unique tool to publicly out people or collectively tar and feather them. Well-meaning people may hope to spread their messages far and wide using Twitter or Facebook, but the fast-spreading messages tend to be sexual, horrific, or humiliating.Gossip is social currency. And in a networked world, trafficking in gossip is far easier than ever before.When someoneâ€™s been wronged â€“ or the opportunity arises to use someone to make a statement â€“ it is relatively easy to leverage social media to incite the hive mind to draw attention to an individual.Â The same tactic that trolls use to target people is the same tactic that people use to out trolls.More often than not, those who use these tools do so when they feel theyâ€™re on the right side of justice. Theyâ€™re either shining a spotlight to make a point or to shame someone into what they perceive to be socially acceptable behavior. But each act of outing has consequences for the people being outed, even if we do not like them or what theyâ€™ve done.This raises serious moral and ethical concerns:Â In a networked society, who among us gets to decide where the moral boundaries lie?This isnâ€™t an easy question and itâ€™s at the root of how we, as a society, conceptualize justice.Governance and the construction of a society is not a fact of life; itâ€™s a public project that we must continuously make and remake. Networked technologies are going to increasingly put pressure on our regulatory structures as conflicting social values crash into one another. In order to benefit from innovation, we must also suffer the destabilizing aspects of new technology.Yet â€¦ that destabilization and suffering allow us, as a society, to interrogate our collective commitments. The hard moral conundrums are just beginning.After falling behind Asia and Europe in the great race, where success is measured in FLOPS (floating-point operations per second), the US has struck back at the new high-tech Olympians with Titan: quite possibly the fastest supercomputer in the world.When Tennesseans hear the word Titan, their first thought is going to be gains on the gridiron, rather than leaps and bounds on the field of science.All of that might now change, as a new supercomputing giant hailing from the Smokey Mountains was unveiled by the US Department of Energyâ€™s (DOE) on Monday.More than 10 times faster and five times more energy efficient than its predecessor Jaguar, Titan is the brainchild of the DOEâ€™s Oak Ridge National Laboratory (ORNL), nestled in the Tennessee highlands.Titanâ€™s theoretical peak is 20 petaflops â€“ 20 quadrillion calculations per second â€“ with 299,008 CPUs (central processing units) and 18,688 graphics processing units (GPUs) spinning at breakneck speeds to make to make scientific breakthroughs in record times.Titan's blistering computation speed will be the equivalent of â€œthe worldâ€™s 7 billion people being able to carry out 3 million calculations per second,â€? ORNL says.Titanâ€™s precursor Jaguar â€“ which was developed by the Seattle-based Cray Inc. â€“ was the fastest supercomputer in the world in June 2010, though it was later outclassed by the Chinese Tianhe-1A several months later.The fastest computer to date is currently the California-based IBM Sequoia, which whirred to the 16.32 petaflops mark in June. Titan also boosts more than 700 terabytes of memory, and will be manage higher energy efficiency than Jaguar by innovatively combining CPUs and the more recent GPUs to synergistic effect.Power limits have long served to trammel those looking to break world records in the great computational race. Jaguarâ€™s 2.3 petaflops needed 7 megawatts of energy â€“ enough to power a small town.At $7 million dollars a year, Jaguarâ€™s electric bill was nothing to scoff at. Titan â€“ essentially an upgraded version of Jaguar housed in the same 200 cabinets arranged very much like a locker room â€“ will hit nearly 10 times the speed while consuming roughly nine megawatts. That makes Titan approximately five times more energy efficient than its previous incarnation.The race to outclass the Chinese and other international competitors has driven Titanâ€™s development forward, though as an open-science system, its benefits will be global."American competitiveness is very important from a global security and national security perspective," Jeffrey Nichols, associate laboratory director for the computing and computational sciences directorate at ORNL, told PCWorld in an interview."It's absolutely important that we are competitive in this high-tech field so the science solutions we are solving are competitive and put us on the leading edge of where we need to be in solving these problems,"he continued.With Titan poised to help the US conduct research in areas like biosciences, climate change, nuclear energy and space, Nichols believes Oak Ridge has â€œdevelopers that can use these machines at scale,â€? while Chinaâ€™s economic development model precludes it from reaching its research potential.But researchers, academics, government labs and a large swath of industries seeking to expedite the scientific method via Titan's ability to use a powerful computational model of varied natural systems are welcome to give it a spin.ORNL has opened its doors to all comers, and 40 projects a year will continue to be given access to the labâ€™s massive computational facilities based on their scientific merits. With Titan, that will mean hundreds of millions CPU hours per project at their disposal.With the never-ending pace of technological development, Titan will inevitably be overthrown by a race of younger computing gods. The Department of Energy already plans on making Titanâ€™s successor operate at 10 times its speed by 2016, meaning Americaâ€™s drive to maintain this golden age of supercomputing excellence might be far from seeing its last day.With its cryptic â€œthe playground is openâ€? tagline, the impending Google Android event has many pundits speculating on what will be introduced. So, we thought weâ€™d once again ask the real experts â€“ consumers â€“ for their take on the Android platform on the eve of what may be the next big thing â€“ or big bust. After all, the nearly 9,000 consumers BIGinsight talks to each month correctly gauged the room temperature reception of Septemberâ€™s iPhone 5 announcement from Apple.As it turns out, a look at the latest results from our â€œHot or Notâ€? feature reveals that the Google Android OS may be becoming quite the pressure cooker for Appleâ€™s iOS. While the majority of adults deemed both the Google Android platform and Apple iOS as pretty popular in October, Android maintained a slight lead on the pairing with 53.0% voting it â€œhotâ€? to Appleâ€™s 51.4%.These insights become really interesting, though, when divvied up by generation. While more than three out of five of the must-have Millennial demographic concurred that both platforms were â€œhot,â€? it was Android again (with 64.0%) that held the edge over Apple (61.9%). The operating system disparity was greatest among Gen X-ers, who were 10% more likely to side with Android (58.6%) versus Apple (53.4%). Boomers were on the fence for this debate, while Apple finally found some support among the Silent generation. Nearly half (46.8%) of those born before 1946 judged Apple to be â€œhot,â€? four points higher than those who felt the same way about Android (41.4%).Bottom Line: While both platforms are undoubtedly popular, it seems that the children of our future â€“ Millenials and Gen X-ers â€“ are positioning Google Android as the mobile future, at least for the time being. As I recall, playground popularity contests could be pretty competitive.Pam Goodfellow (@BIGinsight_Pam) is Consumer Insights Director forBIGinsight. For additional insights, check out this monthâ€™sConsumer Snapshotand theBIG Consumer Blog. And, access complimentary, on-demand insights through one or more of ourInsightCentersâ„¢.The U.S. Department of Energyâ€™s (DOE) Oak Ridge National Laboratory (ORNL)unveiled their new flagship computer, Titan, on Monday. The Department also announced its latest round of Innovative and Novel Computational Impact on Theory and Experiment (INCITE) award recipients.Titan, according to Oak Ridgeâ€™s announcement, is 10 times more powerful than its predecessor, Jaguar, with a theoretical peak performance of 20 petaflops, or 20,000 trillion calculations per second. The current fastest computer, according to the Top 500 list, is Sequoia, which clocked in at 16.32 petaflops in June.The first phase of Titanâ€™s installation was completed earlier this year, and final updates were completed this fall. Titan consumes slightly more energy than Jaguar, but when its significantly faster processing speed is taken into account, it is five times more energy efficient, according to the national laboratoryâ€™s team. The combination of faster speed and only slightly more energy consumption is critical, since the roughly seven megawatts Jaguar consumed â€” enough to power roughly 7,000 homes â€” cost millions. Titan is estimated to consume roughly nine megawatts.â€œThis power problem is changing everything,â€? said Steve Scott, chief technology officer of NVIDIAâ€™s Tesla business unit. â€œThe fact that the energy isnâ€™t dropping as fast as the transistor budget is increasing is just making us more and more power-constrained. And thatâ€™s really whatâ€™s driving us to reinvent how we make processors.â€?In addition to being faster and more efficient, Titan is the same size as its predecessor. Titan, like Jaguar, occupies a space roughly the size of a basketball court, with each stack approximately the size of a household kitchen refrigerator. Thatâ€™s due to the nature of the upgrade, which primarily involved the incorporation of graphic processing unit (GPU) accelerators. GPUs are primarily used for computer games, but can be used to accelerate central processing units, or CPUs.Titan is a Cray-XK7 system and is the first machine to use NVIDIAâ€™s latest GPU accelerator, the Tesla K20, with each of Titanâ€™s 18,688 nodes holding one CPU and GPU accelerator, according to Oak Ridge and NVIDIA. The GPU used in Titan is no different, said Scott, than the one made for high-end gaming units.â€œThe technology for gaming is the disruptive technology thatâ€™s now impacting computing broadly,â€? said Jeff Nichols, associate director of Oak Ridge National Laboratory.Pairing GPUs and CPUs in and of itself is not novel, but â€œthere were a lot of skepticsâ€? at Oak Ridge, said Scott. It had never been done on this scale before, and Titan had to be more than, in Scottâ€™s words, a â€œstunt.â€? The machine had to be able to run six predetermined applications to Oak Ridgeâ€™s specifications. The programs are in the areas of material science, climate change, biofuels, astrophysics, combustion and nuclear energy.Discovers using Titan could have an impact by leading to cleaner, more efficient engines, faster and cheaper drug testing, climate modeling and even the development of future high-performance computers.â€œThis opens up new vistas of calculations we couldnâ€™t conceive of doing before,â€? said Jeremy Smith, Governorâ€™s Chair at the University of Tennessee and also director of the Center for Molecular Biophysics at Oak Ridge National Laboratory. Smith is likely to be one of the most frequent users of Titan, as he was with Jaguar. But Smith emphasized that Titan is merely one step in high-performance computer evolution. Smith is among many who await the arrival of exascale computing, which, in theory, would allow for, among other things, the simulation of a living cell in atomic detail.â€œItâ€™s really what comes afterwards that will provide the bulk of the discoveries,â€? said Smith. â€œWhat Titan will have done is to set the standard in computer power, identify the challenges in using such a machine, and it will make a couple of useful discoveries that couldnâ€™t be made on any other machine.â€?Smith will not be alone in leveraging Titanâ€™s processing power. Recipients of the 2013 INCITE awards will also have access to Titan. The Department of Energyâ€™s Leadership Computing Facilities (LCFs) awarded a total of 4.7 billion hours to 61 projects in science and engineering â€” 1.84 billion hours on Titan and 2.83 billion hours on two of Argonne National Laboratoryâ€™s supercomputers, Mira and Intrepid. Projects ranged from research around nuclear reactors and electric engines to the development of a unified theory for physical forces.Titanâ€™s public unveiling comes weeks before the release of the latest Top500 supercomputer rankings. The Top500 list, which dates back to 1993, is released twice a year â€” once in June and again in November. The Titan team anticipates their machine will be ranked in the top two, which would make it the fastest high-performance computer open to non-classified projects. Thatâ€™s assuming Sequoia comes in first or second. Sequoia is housed at Lawrence Livermore National Laboratory (LLNL) and is used exclusively by the National Nuclear Security Administration (NNSA) to manage the United Statesâ€™ nuclear weapons stockpile. Jaguar ranked sixth in the latest list.When asked what he would use Titan for, Nichols said he thought there was â€œfascinatingâ€? research to be done in chemical physics, specifically simulating the breaking of chemical bonds. But given his focus in materials science, he said he would most likely use Titan to figure out how to design better photovoltaics.NVIDIAâ€™s Scott, on the other hand, said he would use the supercomputer to help him find Titanâ€™s successor. But a few moments later, added, â€œYou could also maybe use the machine for a really giant multi-user game.â€?Read more news and ideas on Innovations:New database grades lawmakers on their tech-friendlinessâ€˜Iron Manâ€™-style exoskeleton could help in space and here on EarthThe network made it official today that production on the long-running shows is shutting down at the end of the year. Attack Of The Show and X-Play helped define G4â€˜s gamer-culture focus and launched careers for the likes of Olivia Munn and Chris Hardwick, amassing close to 3000 episodes to date. They also provided wall-to-wall coverage of Comic-Con and E3 â€” two events right in the networkâ€™s young-male demo wheelhouse. X-Play launched in 2003 on what was then known as TechTV; Attack followed in 2005. Hereâ€™s the networkâ€™s release about farewell plans for the shows, which will air original episodes through 2012:Los Angeles, CA, October 26, 2012 â€“ Attack of the Show! and X-Play are the longest-running and defining series for G4 through its first decade. With the shows ending production at the end of 2012, G4 is getting set to showcase the landmark series as they wind down their long runs on the network.With upwards of 1,700 and 1,300 episodes, respectively, Attack of the Show! and X-Play defined the gamer culture for a generation of young men, and served as the launch pad for prominent personalities including Kevin Pereira, Olivia Munn, Chris Hardwick and Adam Sessler. Guests James Cameron, Ryan Reynolds, Jimmy Fallon, William Shatner, Sasha Baron Cohen and Joseph Gordon-Levitt are among the notables who got their geek on. The shows also pioneered live-from-the-floor coverage of the two most important conventions in the game culture universe: San Diego Comic-Con and E3.Leading up to their final episodes, Attack of the Show! and X-Play will look back at their most memorable moments, important scoops, entertaining programming and appealing hosts. A rotating lineup of guest co-hosts like John Barrowman, Michael Ian Black, Josh Myers, Paul Scheer, Rob Huebel and Horatio Sanz will join AOTS hosts Candace Bailey and Sara Underwood, and X-Play hosts Morgan Webb and Blair Herter as part of the farewell shows.â€œAttack of the Show! and X-Play have been important for G4, and we want to acknowledge the creative people who have helped inspire and showcase the phenomenon of gamer culture,â€? G4 Media General Manager Adam Stotsky said. â€œWith more than 3,000 episodes aired between them, we have more than enough great material to honor these innovators and their amazing contributions as we bring both shows to a close.â€?Attack of the Show! debuted March 28, 2005 and from the start was the ultimate male guide to everything cool and new in the world of technology, web culture, gaming and pop culture. For the next few months, AOTS will mix new segments with audience favorites, such as the iPhone extravaganza on June 29, 2007, on the eve of the debut of the first generation of Appleâ€™s market-changing smartphone. The July 2006 premiere of the first live-from-the-floor coverage from San Diego Comic-Con will be celebrated as well. Old friends will return to join the celebration, and the showâ€™s signature cheeky attitude and feel-for-the zeitgeist will be very much in evidence.X-Play made its debut almost two years earlier, on April 28, 2003 (on G4â€™s previous incarnation: TechTV), and immediately became the go-to destination for young men seeking the latest video game news, honest reviews, hands-on demos and exclusive video game trailers and footage. The year-end celebration will take viewers back through highlights of this landmark showâ€™s history, including its exclusive live-from-the-floor coverage of the E3 convention in Los Angeles, the most important annual gathering for the gaming community. X-Play has also established a franchise of doing an annual year-end round-up of the best in a wide range of video games, and you can be sure those will be revisited before the show signs-off for good. As with AOTS, expect old friends to return too.Attack of the Show! and X-Play will air original episodes through the end of the year.A core goal for Ubuntu 13.04 is to get Ubuntu running on a Nexus 7 tablet. To be clear, this is not going to be a tablet Unity interface running on the 8/16GB Nexus 7, but instead will focus on getting the current Ubuntu Desktop running on the Nexus so that we can ensure pieces such as the kernel, power management and other related areas are working effectively on a tablet device.Topics such as battery life, memory footprint, and support for sensors are all areas in which needs and expectations vary widely between a PC and a mobile devices. The 13.04 cycle will very much be focused on this exploration and learning and this is why we want to focus our efforts on getting the existing Ubuntu Desktop running on the Nexus 7. This will mean that some user-facing parts of the experience wonâ€™t make a lot of sense on the tablet, but we want to get the foundations optimized before we focus on these higher level challenges.Naturally we want our community to be involved throughout this exploration and I want to talk more about how you can get involved both as a tester and as a developer.To help with testing you will need an 8/16GB Nexus 7 tablet and be willing to replace the Android Operating System with Ubuntu (as such, please be sure to back up any valuable data on your tablet).You can follow instructions of how to install Ubuntu on your device by reading the instructions at https://wiki.ubuntu.com/Nexus7.If you have any questions about the installation and setup, please post on Ask Ubuntu; we will use the mobile tag to track these questions. The Mobile development team will be regularly monitoring the questions, and we would like you folks to help answer the list of questions too if you have the answers.When you find bugs, please use to file the bug (more details about using can be found here). Please also tag the bug with so we can find them more easily.You can also get in touch with our wider testing community in #ubuntu-testing on the Freenode IRC network.If you are interested in contributing to making Ubuntu work flawlessly and optimizing the Ubuntu Desktop core for the Nexus 7, we would love to have you participate in this work.You can find details of many of the areas that we would like to focus on over at Victorâ€™s blog; this provides some great food for thought for performance and functionality goals.Much of this work will be discussed at the upcoming Ubuntu Developer Summit taking place in Copenhagen from 29th Oct â€“ 1st Nov 2012.If you are unable to participate in person you can join the sessions remotely. For instructions of how to participate remote, see this page for instructions. You are also encouraged to join #ubuntu-arm on Freenode to discuss this work.The following sessions are scheduled. Please note times may change, so be sure to click the link below to ensure the date/time is up to date. You can also find the appropriate blueprints linked from the links below too:Made in IBM Labs: Researchers Demonstrate Initial Steps toward Commercial Fabrication of Carbon Nanotubes as a Successor to Silicon ï‚· For the first time, scientists precisely place and test more than ten thousand carbon nanotube devices in a single chip using mainstream manufacturing processes ï‚· Novel processing method helps pave the way for carbon technology as a viable alternative to silicon in future computingYORKTOWN HEIGHTS, NY â€“ 28 Oct 2012: IBM (NYSE: IBM) scientists have demonstrated a new approach to carbon nanotechnology that opens up the path for commercial fabrication of dramatically smaller, faster and more powerful computer chips. For the first time, more than ten thousand working transistors made of nano-sized tubes of carbon have been precisely placed and tested in a single chip using standard semiconductor processes. These carbon devices are poised to replace and outperform silicon technology allowing further miniaturization of computing components and leading the way for future microelectronics.Aided by rapid innovation over four decades, silicon microprocessor technology has continually shrunk in size and improved in performance, thereby driving the information technology revolution. Silicon transistors, tiny switches that carry information on a chip, have been made smaller year after year, but they are approaching a point of physical limitation. Their increasingly small dimensions, now reaching the nanoscale, will prohibit any gains in performance due to the nature of silicon and the laws of physics. Within a few more generations, classical scaling and shrinkage will no longer yield the sizable benefits of lower power, lower cost and higher speed processors that the industry has become accustomed to.Carbon nanotubes represent a new class of semiconductor materials whose electrical properties are more attractive than silicon, particularly for building nanoscale transistor devices that are a few tens of atoms across. Electrons in carbon transistors can move easier than in silicon-based devices allowing for quicker transport of data. The nanotubes are also ideally shaped for transistors at the atomic scale, an advantage over silicon. These qualities are among the reasons to replace the traditional silicon transistor with carbon â€“ and coupled with new chip design architectures â€“ will allow computing innovation on a miniature scale for the future.The approach developed at IBM labs paves the way for circuit fabrication with large numbers of carbon nanotube transistors at predetermined substrate positions. The ability to isolate semiconducting nanotubes and place a high density of carbon devices on a wafer is crucial to assess their suitability for a technology â€“ eventually more than one billion transistors will be needed for future integration into commercial chips. Until now, scientists have been able to place at most a few hundred carbon nanotube devices at a time, not nearly enough to address key issues for commercial applications."Carbon nanotubes, borne out of chemistry, have largely been laboratory curiosities as far as microelectronic applications are concerned. We are attempting the first steps towards a technology by fabricating carbon nanotube transistors within a conventional wafer fabrication infrastructure," said Supratik Guha, Director of Physical Sciences at IBM Research. "The motivation to work on carbon nanotube transistors is that at extremely small nanoscale dimensions, they outperform transistors made from any other material. However, there are challenges to address such as ultra high purity of the carbon nanotubes and deliberate placement at the nanoscale. We have been making significant strides in both."Originally studied for the physics that arises from their atomic dimensions and shapes, carbon nanotubes are being explored by scientists worldwide in applications that span integrated circuits, energy storage and conversion, biomedical sensing and DNA sequencing.This achievement was published today in the peer-reviewed journal Nature Nanotechnology.Carbon, a readily available basic element from which crystals as hard as diamonds and as soft as the "lead" in a pencil are made, has wide-ranging IT applications.Carbon nanotubes are single atomic sheets of carbon rolled up into a tube. The carbon nanotube forms the core of a transistor device that will work in a fashion similar to the current silicon transistor, but will be better performing. They could be used to replace the transistors in chips that power our data-crunching servers, high performing computers and ultra fast smart phones.Earlier this year, IBM researchers demonstrated carbon nanotube transistors can operate as excellent switches at molecular dimensions of less than ten nanometers â€“ the equivalent to 10,000 times thinner than a strand of human hair and less than half the size of the leading silicon technology. Comprehensive modeling of the electronic circuits suggests that about a five to ten times improvement in performance compared to silicon circuits is possible.There are practical challenges for carbon nanotubes to become a commercial technology notably, as mentioned earlier, due to the purity and placement of the devices. Carbon nanotubes naturally come as a mix of metallic and semiconducting species and need to be placed perfectly on the wafer surface to make electronic circuits. For device operation, only the semiconducting kind of tubes is useful which requires essentially complete removal of the metallic ones to prevent errors in circuits. Also, for large scale integration to happen, it is critical to be able to control the alignment and the location of carbon nanotube devices on a substrate.To overcome these barriers, IBM researchers developed a novel method based on ion-exchange chemistry that allows precise and controlled placement of aligned carbon nanotubes on a substrate at a high density â€“ two orders of magnitude greater than previous experiments, enabling the controlled placement of individual nanotubes with a density of about a billion per square centimeter.The process starts with carbon nanotubes mixed with a surfactant, a kind of soap that makes them soluble in water. A substrate is comprised of two oxides with trenches made of chemically-modified hafnium oxide (HfO2) and the rest of silicon oxide (SiO2). The substrate gets immersed in the carbon nanotube solution and the carbon nanotubes attach via a chemical bond to the HfO2 regions while the rest of the surface remains clean.By combining chemistry, processing and engineering expertise, IBM researchers are able to fabricate more than ten thousand transistors on a single chip. Furthermore, rapid testing of thousands of devices is possible using high volume characterization tools due to compatibility to standard commercial processes.As this new placement technique can be readily implemented, involving common chemicals and existing semiconductor fabrication, it will allow the industry to work with carbon nanotubes at a greater scale and deliver further innovation for carbon electronics.A memory leak and a failed monitoring system caused theÂ Amazon Web Services outage on Monday that took out Reddit and other major services.According to a postFriday night, AWS explained that the problem arose after a simple replacement of a data collection server. After installation, the server did not propagate its DNS address correctly and so a fraction of servers did not get the message. Those servers kept trying to reach the server, which led to a memory leak that then went out of control due to the failure of an internal monitoring alarm. Eventually the system groundÂ to a virtual stop and millions of customers felt the pain.The failure in its North Virginia region eventually interrupted Reddit, Foursquare, Minecraft, Heroku, GitHub, imgur, Pocket, HipChat, Coursera and a number of others.In the past, Amazonâ€™s Elastic Block Storage (EBS) servers have proved troublesome. This outage proved not much different. The EBS servers, feeling the memory leak, began losing the ability to process customer requests, causingÂ the number of stuck volumes to increase quickly. The server degradation came all at once, causing a tax on the system as not enough healthy servers could be found to replace them all.The outage started at 10 a.m. PST. Five hours later, AWS discovered the root of the problem. An hour later things got back to normal.AWS says it is taking a number of steps to prevent similar issues going forward. The group plans to deploy monitoring that will sound the alarm if this specific memory leak problem arises again in any of its production EBS servers. Next week it will begin deploying a fix for the memory leak issue.AWS has had its share of outages over the past several months. Its problems are magnified by an increasingly competitive market that is seeking to slow AWSâ€™ momentum by casting doubt on its infrastructure.I get the competitive issues in play here. But customers should not overlook AWSâ€™ uniqueness in providing a service that allows startups to use elastic computing, network and storage to compete on the world stage. It may have outages, but no other service comes even close to what AWS offers its customers.In the final phase of its first operational flight, the commercial cargo ship returns safely to Earth, carrying nearly a ton of supplies and experiment samples.In a major milestone for the space station program, a commercial cargo capsule loaded with nearly a ton of long-awaited experiment samples, broken components, and other gear returned to Earth on Sunday, plunging back through the atmosphere to a Pacific Ocean splashdown and wrapping up the spacecraft's first operational flight.The SpaceX Dragon capsule is the first space station cargo ship since the shuttle capable of carrying large amounts of equipment both to and from the lab complex. As such, it restores a critical capability for NASA -- the return of experiment samples from the station -- along with failed components that require troubleshooting and analysis."We see her moving aft and away from us out of the keep-out sphere," Expedition 33 Commander Sunita Williams radioed from the station as the Dragon capsule departed early today. "It was nice while she was on board. We tamed her, took her (on board), and literally and figuratively, there's a piece of us on that spacecraft going home to Earth."She was referring to urine and other biological samples packed aboard the cargo ship that had been awaiting a ride back to researchers on the ground."Not only is it going to give us a consistent supply chain up, but very critical, particularly to biological research, is the return mass, to be able to have frozen samples returned home," space station Program Manager Mike Suffredini said earlier. "This really is the keystone to what is going to allow the space station to do what it was built to do. It's critical to the success of the station."Designed, built, and operated by Space Exploration Technologies -- SpaceX -- under a $1.6 billion contract with NASA, the Dragon capsule was launched from Cape Canaveral, Fla., on October 7, loaded with nearly a half ton of supplies and equipment. It was captured by the station's robot arm three days later and attached to the Earth-facing port of the forward Harmony module.After unloading its cargo, the station crew repacked the capsule with nearly a ton of experiment samples, station components, and other gear awaiting return to Earth. Williams and Japanese astronaut Akihiko Hoshide, operating the space station's robot arm, detached Dragon from its berthing port at 7:19 a.m. ET today. The astronauts then released the capsule at 9:29 a.m. as the two spacecraft sailed 255 miles above Burma.At that point, SpaceX flight controllers in Hawthorne, Calif., took over active control, using thruster firings to move the capsule away from the space station. At 2:28 p.m., the capsule's braking rockets fired for 9 minutes and 50 seconds, dropping the far side of its orbit deep into the atmosphere over the Pacific Ocean.After enduring the heat of re-entry, the capsule's two drogue parachutes deployed at an altitude of about 45,000 feet, slowing the craft enough to permit the release of three large main parachutes at an altitude of around 10,000 feet. A SpaceX team was standing by in the landing zone 250 miles off the coast of Baja California to recover the spacecraft."The SpaceX recovery boat sees the vehicle with three main chutes out," NASA mission control radioed the station crew at 3:16 p.m."Good news," Williams replied from orbit. "Thanks for the update."A few moments later, at 3:22 p.m., the spacecraft splashed into the Pacific Ocean to complete the return to Earth."Station, Houston on two, Dragon is in the Pacific," mission control advised."Awesome," Williams said. "She made it home to Earth."The SpaceX commercial resupply contract requires the company to deliver 44,000 pounds of equipment and supplies over 12 flights. To pave the way for operational resupply missions, SpaceX carried out two successful test flights, one that tested the capsule's systems in a solo flight and another that included a berthing at the station last May.The Dragon capsule measures 14.4 feet tall and 12 feet wide, with a trunk section, jettisoned just before re-entry, that extends another 9.2 feet below the capsule's heat shield and houses two solar arrays and an unpressurized cargo bay. The spacecraft can carry up to 7,297 pounds of cargo split between the pressurized and unpressurized sections.Under a separate $440 million contract with NASA, SpaceX engineers are working on upgrades to convert the Dragon capsule into a manned spacecraft that can ferry crews to and from the station. SpaceX managers believe they will be ready for initial manned test flights in the 2015 timeframe, assuming continued NASA funding. Two other companies, Boeing and Sierra Nevada, are developing their own spacecraft designs under similar contracts.For Dragon's first Commercial Resupply Service mission -- CRS-1 -- the SpaceX cargo capsule delivered 882 pounds of hardware, supplies, and equipment to the space station, including 260 pounds of crew food and supplies, 390 pounds of science gear, and 225 pounds of spare parts and other station hardware.For its return to Earth, the Dragon was packed with about 1,673 pounds of experiment samples and hardware, including 163 pounds of crew supplies; 518 pounds of station hardware; 123 pounds of computer gear and Russian equipment; and 866 pounds of science gear and experiment samples.Update: This story was originally published at 8:06 a.m. PT after the Dragon left the space station. It was updated at 12:50 p.m. PT with information and quotations regarding re-entry and splashdown.Microsoft's biggest desire is to get you using Windows 8, and fast. Here's how to use that $40 upgrade to flip older versions of Windows to Windows 8.The Windows 8 eagle has landed, which means that Microsoft's $39.99 in-place upgrade is now available. They've made it extremely easy to upgrade your computer from a Windows 7, Vista, or XP computer to Windows 8. Here's how it's done.First, check out our CNET guide on how to prepare your computer for Windows 8. There's also instructions on how to restore your old system, which is important in case something unexpectedly goes awry, or you decide you don't like Windows 8, you can restore what you had before.If you're upgrading a laptop, you must take note of your Wi-Fi passwords. Windows 8 will keep your settings, personal files, and programs if you upgrade from Windows 7. Vista and XP upgraders will have to re-install programs and reconfigure settings.Next, go to Microsoft's Windows 8 site. Scroll down to the offer and click "Get the details." It's $39.99 for the downloadable installer, or $69.99 to have them mail you a disc.When you click on the Download Pro link, it will give to your computer a small "stub" installer. It's a 5 MB file that will run a compatibility check on your computer, tell you which programs will and won't work in Windows 8, and let you know if you have to uninstall any of them.Until Windows 8 begins its installation, there's some babysitting required. After running the stub, it'll tell you how much of your current computer is compatible, and if there's anything you'll have to review. On the Toshiba Satellite running Windows 7 that I tested this on, I learned that Windows 8 is about as fond of bloatware as the rest of us: the upgrade process requested that I uninstall several Toshiba-branded programs.You don't have to stop everything to take care of them, though, because the Windows 8 installer will walk you through that process. After letting you know whether there are details that will require your attention, it asks Windows 7 upgraders what they'd like to keep of their settings, apps, and personal files.After that, it asks you to buy the upgrade. The ordering process happens in the installer, too. Once you've paid by either credit card or PayPal, it will start downloading the 2 GB installer. You get a choice of installing immediately, installing later, or creating a USB key or disc from which to run the installer.After that, it prompts you to remove any programs that cause conflicts on Windows 8, such as the aforementioned Toshiba-built software. It's clearly a nuanced process, though, as I wasn't required to uninstall all of it -- just four programs out of 11 conflicts. If you have to restart your computer, just double-click on the Windows 8 installation icon on your desktop and it will quickly find where it left off.Once any conflicts are eliminated, Windows 8 will install. It's a surprisingly fast process, as long as you remember to babysit it at the beginning.SAN FRANCISCO â€” I.B.M. scientists are reporting progress in a chip-making technology that is likely to ensure that the basic digital switch at the heart of modern microchips will continue to shrink for more than a decade.The advance, first described in the journal Nature Nanotechnology on Sunday, is based on carbon nanotubes â€” exotic molecules that have long held out promise as an alternative to silicon from which to create the tiny logic gates now used by the billions to create microprocessors and memory chips.The I.B.M. scientists at the T.J. Watson Research Center in Yorktown Heights, N.Y., have been able to pattern an array of carbon nanotubes on the surface of a silicon wafer and use them to build hybrid chips with more than 10,000 working transistors.Against all expectations, silicon-based chips have continued to improve in speed and capacity for the last five decades. In recent years, however, there has been growing uncertainty about whether the technology would continue to improve.A failure to increase performance would inevitably stall a growing array of industries that have fed off the falling cost of computer chips.Chip makers have routinely doubled the number of transistors that can be etched on the surface of silicon wafers by shrinking the size of the tiny switches that store and route the ones and zeros that are processed by digital computers.The switches are rapidly approaching dimensions that can be measured in terms of the widths of just a few atoms.The process known as Mooreâ€™s Law was named after Gordon Moore, a co-founder of Intel, who in 1965 noted that the industry was doubling the number of transistors it could build on a single chip at routine intervals of about two years.To maintain that rate of progress, semiconductor engineers have had to consistently perfect a range of related manufacturing systems and materials that continue to perform at evermore Lilliputian scale.The I.B.M. advance is significant, scientists said, because the chip-making industry has not yet found a way forward beyond the next two or three generations of silicon.â€œThis is terrific. Iâ€™m really excited about this,â€? said Subhasish Mitra, an electrical engineering professor at Stanford who specializes in carbon nanotube materials.The promise of the new materials is twofold, he said: carbon nanotubes will allow chip makers to build smaller transistors while also probably increasing the speed at which they can be turned on and off.In recent years, while chip makers have continued to double the number of transistors on chips, their performance, measured as â€œclock speed,â€? has largely stalled.This has required the computer industry to change its designs and begin building more so-called parallel computers. Today, even smartphone microprocessors come with as many as four processors, or â€œcores,â€? which are used to break up tasks so they can be processed simultaneously.I.B.M. scientists say they believe that once they have perfected the use of carbon nanotubes â€” sometime after the end of this decade â€” it will be possible to sharply increase the speed of chips while continuing to sharply increase the number of transistors.This year, I.B.M. researchers published a separate paper describing the speedup made possible by carbon nanotubes.â€œThese devices outperformed any other switches made from any other material,â€? said Supratik Guha, director of physical sciences at I.B.M.â€™s Yorktown Heights research center. â€œWe had suspected this all along, and our device physicists had simulated this, and they showed that we would see a factor of five or more performance improvement over conventional silicon devices.â€?Carbon nanotubes are one of three promising technologies engineers hope will be perfected in time to keep the industry on its Mooreâ€™s Law pace. Graphene is another promising material that is being explored, as well as a variant of the standard silicon transistor known as a tunneling field-effect transistor.Dr. Guha, however, said carbon nanotube materials had more promising performance characteristics and that I.B.M. physicists and chemists had perfected a range of â€œtricksâ€? to ease the manufacturing process.Carbon nanotubes are essentially single sheets of carbon rolled into tubes. In the Nature Nanotechnology paper, the I.B.M. researchers described how they were able to place ultrasmall rectangles of the material in regular arrays by placing them in a soapy mixture to make them soluble in water. They used a process they described as â€œchemical self-assemblyâ€? to create patterned arrays in which nanotubes stick in some areas of the surface while leaving other areas untouched.Perfecting the process will require a more highly purified form of the carbon nanotube material, Dr. Guha said, explaining that less pure forms are metallic and are not good semiconductors.Dr. Guha said that in the 1940s scientists at Bell Labs had discovered ways to purify germanium, a metal in the carbon group that is chemically similar to silicon, to make the first transistors. He said he was confident that I.B.M. scientists would be able to make 99.99 percent pure carbon nanotubes in the future.This post has been revised to reflect the following correction:Because of an editing error, an article on Monday about an I.B.M. breakthrough on chip design defined incorrectly Mooreâ€™s Law, an observation on technology advances named for Gordon Moore, a co-founder of Intel. Mooreâ€™s Law holds that the chip industry doubles the number of transistors it can build on a single chip at routine intervals of about two years â€” not intervals of about 12 to 18 months.Freemium app revenue is now dominating premium for developers on both iOS and Android, said App Annie. The analytics firm said that freemium apps generate 69 percent of the worldwide iOS app revenue and 75 percent of global Android app revenues.The mobile app world took to the freemium model with a passion last year, as revenue from freemium iOS appseclipsed 50 percent mark in the US about a year ago. But in the last year, the momentum behind freemium apps has only grown stronger, according to new data from app analytics firm App Annie.App Annie Intelligence, which tracks more than 700,000 apps, found that global revenues for freemium apps on iOS have quadrupled over the last 24 months. And for Google Play, worldwide freemium revenues have grown 3.5x in 2012. Now, freemium apps generate 69 percent of the worldwide iOS app revenue and 75 percent of global Android app revenues.Â Meanwhile, premium app revenue from paid download apps have remained relatively flat over the same periods.The numbers confirm the trend weâ€™ve been noticing but the fact that thereâ€™s been no let up shows just how app developers continue to embrace the freemium model and how those apps continue to bring in more money. WeÂ reported two years agoÂ that the 1/3 of the top grossing apps on iOS in the US had moved to the freemium model. By the end of 2011,Â Distimo reportedÂ that about half of the revenue from the 200 top grossing iPhone apps came from freemium app while 65 percent of the revenue from top apps in the Android Market came from freemium apps. Hereâ€™s a look at some of the charts worked up by App Annie Intelligence:Â In January,Â IHS saidÂ that in-app purchase in freemium apps brought in $970 million in worldwide sale last year, or 39 percent compared to paid downloads. And freemium app revenue was expected to grow to $5.6 billion by 2015, representing 64 percent of the total market. The App Annie data, which is limited to iOS and Android, suggests we may be on a faster pace than IHS predicted.Itâ€™s not just in the US, where the figures generally mirror the world stats. App Annie said countries like China and Japan have rapidly adopted the freemium model in the last year. Japanese freemium revenues grew by 24x in the last year on Google Play and Chinese freemium revenue grew by nearly 25x on iOS since January 2011.Not every app needs to go freemium. AsÂ Flurry recently pointed out, some apps are better suited to that model. For example, apps with high intensity of usage in a short window creates an opportunity for developers to make money though in-app purchases that users can binge on. And for users who come back repeatedly over a long period of time, thereâ€™s also a chance to keep selling them on more content and add-on functions. Apps that donâ€™t necessarily hold on to users over a long period of time might monetize better through one-time paid downloads, said Flurry.I suspect weâ€™ll see paid downloads remain as a viable option for some developers. Instapaperâ€™s success, for example, has shown thatÂ consumers will pay up front for a good product. But increasingly, the bigger money seems to be found in letting people in for free and then monetizing a smaller group of users over time through in-app purchases, subscriptions and other added features.Streaming video may be the epitome of instant gratification, but that's only after you actually find the content you're looking for. You may know you want to watch "True Grit," but it's not easy to remember whether it's available on Netflix, Amazon Instant, or any of the other countless services available.That's exactly the problem the newly announced Roku Search is designed to fix.The feature upgrade will allow owners of Roku boxes to search for TV and movies available on Netflix, Amazon Instant, Vudu, Hulu Plus, Crackle, and HBO Go via one simple interface. The Roku Search channel will be rolling out to supported players (Roku LT, Roku HD and Roku 2 boxes) over the week and sits right next to the settings menu on Roku's home screen.I had the opportunity to try out the new feature over the weekend and it works well. Search for a movie and Roku will show you which services it's available for, plus whether it's free for subscribers or requires a pay-per-view fee. After you select your service, Roku launches the appropriate channel, bringing you right to the content you selected. The most tedious part is entering text, although you can use Roku's smartphone app (available for iOS and Android) to speed up the process. It's also possible to search for actors and directors and browse their available content.There are a few quirks. While you can use the Roku smartphone app to enter in text, response time is slightly delayed. I found I had to purposefully type slower than usual, otherwise letters would occasionally be left out, despite typing correctly. I also noticed that Roku Search would only specifically note that certain Amazon Instant movies are available to stream free for Prime members; TV shows didn't have the same "free for Prime" designation.Overall, it's definitely a handy feature, albeit one that only helps if you know what you want to watch. Competing platforms like the Xbox 360 and Google TV offers cross-platform browsing, in addition to search, which can help you find content you don't already know you want to watch. But in the back-and-forth battle between Roku and the Apple TV, it's another win for Roku, as Apple's box doesn't offer any cross-platform searching capabilities.Weâ€™ve seen some absurd trademark threats in recent years, but this one sets the bar at a new low: The Village Voice is suing Yelp for trademark infringement based on Yelpâ€™s creation of various â€œBest ofâ€? lists.  Yes, that's correct, the publisher behind the paper (as well as several other weeklies around the U.S.) has managed to register trademarks in the term â€œBest of â€? in connection with several cities, including San Francisco, Miami, St. Louis and Phoenix.   And it now claims that Yelpâ€™s use of those terms infringes those trademarks and deceives consumers.Right.First, a practical question: deceives consumers about what?  Trademark law is supposed to ensure that consumers can trust that the goods and services they buy come from the sources they expect, e.g., that the Pepsi you just bought really was manufactured by Pepsi.  That helps consumers, because it gives mark-owners an incentive to maintain the expected level of quality. And it helps mark-owners, because they can build customer loyalty and good will.   But you donâ€™t need a survey or even a lawyer to figure out that no one actually thinks the Village Voice is associated with Yelp because both publish â€œbest ofâ€? lists â€“ not least because no one associates the term â€œBest ofâ€? with any particular news source. Second, the more important question: What is going on at the Patent and Trademark Office?  For decades, folks have been complaining (with good reason) that the patent examiners need to do a better job of screening out bogus patent applications.  Itâ€™s clear that the problem extends to the trademark side as well. The PTO has allowed companies and individuals to register marks in any number of obviously generic and/or descriptive terms, such as â€œurban homesteadâ€? (to refer to urban farms), â€œgaymerâ€? (to refer to gay gamers), and â€œB-24â€? (to refer to model B-24 bombers).Once a mark is registered, it is all too easy for the owner to become a trademark bully.  And while companies like Yelp have the resources to fight back (as we expect it will), small companies and individuals may not. Just as dangerous, the trademark owner may go upstream, to intermediaries like Facebook who have little incentive to do anything other than take down an account or site thatâ€™s accused of infringement. "Good enough for government work" isn't good enough for free speech. Itâ€™s time the PTO did its part to stop trademark bullies and tightened up the trademark application process. Fewer bogus registrations means fewer bogus threats, and more online creativity and competition.  That's a win for everyone.In case you havenâ€™t seen the news yet, earlier today, AMD made an announcement that represents a new era in the compute landscape and builds on our rich tradition of bringing disruptive technology to the data center. By announcing our intent to build 64-bit ARM technology-based server CPUs, AMD has embarked on a path that will effectively end the one size fits all compute era that has dominated the data center for the past two decades.With the explosion of new devices and business models that touch the internet, the data center has now become the center of the universe.Â  New workloads are placing tremendous demands on the server infrastructure which is forcing the need for accelerating the pace of innovation.Â  The largest data centers in the world are adding compute at an extremely fast pace, and the market is looking for disruptive ways to improve the efficiency and reduce the total cost of ownership in the data center.Small and efficient CPU, like ARM CPUs, bring a very unique capability to the data center.Â  Â Â The compute/$$ and compute/watt is substantially improved in ARM CPUs over large-core CPUs, thus making them ideal for highly parallelizable tasks.Â  However, the challenge with efficient CPUs is that they need to be linked to the network.Â Â  If each individual efficient CPU is linked to the network, it becomes a very inefficient way to operate.This is where AMD can come in and can offer a unique advantage to drive the industry forward. Â Â AMD, through our SeaMicro acquisition, has the industryâ€™s premier fabric, the AMD SeaMicro Freedomâ„¢ fabric.Â  By using Freedom fabric to link ARM-based CPUs into a cluster, and then linking the clusters to the network, AMD can effectively solve the bottleneck of leveraging small, efficient CPUs in the mega data centers of tomorrow.There are a number companies that have talked about 64-bit ARM in the server space but only AMD comes in with the background and expertise to drive and accelerate the ARM 64-bit ecosystem. We start with a deep knowledge of what it takes to be successful in servers; industry-leading 64-bit microprocessor technology, a broad portfolio of IP and the experience of working with OEMs, ODMs and ISVs to really deliver an enterprise-class portfolio of features. In fact, at our press event in San Francisco earlier today, we pulled together a panel of industry leaders from Facebook, Dell, Red Hat, Amazon and ARM to talk about this movement to more flexible and energy-efficient compute solutions and what was required to further drive this data center inflection point.The best part of it is that we provide customers choice.Â  In addition to ARM-based CPUs, AMD will continue to offer x86 CPUs and APUs so that customers can use the right processor for the right workload.We are extremely excited to be driving the industry at this key inflection point in the data center.Â  This is an exciting time to be in our industry and we look forward to partnering with the entire ecosystem to drive this disruptive change.Take a look at the video here for highlights of todayâ€™s panel and event.Lisa Su is senior vice president and general manager of AMDâ€™s Global Business Units.Her postings are her own opinions and may not represent AMDâ€™s positions, strategies or opinions. Links to third party sites, and references to third party trademarks, are provided for convenience and illustrative purposes only. Unless explicitly stated, AMD is not responsible for the contents of such links, and no third party endorsement of AMD or any of its products is implied.OAK RIDGE, Tenn., Oct. 29, 2012 Â— The U.S. Department of Energy's (DOE) Oak Ridge National Laboratory launched a new era of scientific supercomputing today with Titan, a system capable of churning through more than 20,000 trillion calculations each secondâ€”or 20 petaflopsâ€”by employing a family of processors called graphic processing units first created for computer gaming. Titan will be 10 times more powerful than ORNL's last world-leading system, Jaguar, while overcoming power and space limitations inherent in the previous generation of high-performance computers.Titan, which is supported by the Department of Energy, will provide unprecedented computing power for research in energy, climate change, efficient engines, materials and other disciplines and pave the way for a wide range of achievements in science and technology.The Cray XK7 system contains 18,688 nodes, with each holding a 16-core AMD Opteron 6274 processor and an NVIDIA Tesla K20 graphics processing unit (GPU) accelerator. Titan also has more than 700 terabytes of memory. The combination of central processing units, the traditional foundation of high-performance computers, and more recent GPUs will allow Titan to occupy the same space as its Jaguar predecessor while using only marginally more electricity."One challenge in supercomputers today is power consumption," said Jeff Nichols, associate laboratory director for computing and computational sciences. "Combining GPUs and CPUs in a single system requires less power than CPUs alone and is a responsible move toward lowering our carbon footprint. Titan will provide unprecedented computing power for research in energy, climate change, materials and other disciplines to enable scientific leadership."Because they handle hundreds of calculations simultaneously, GPUs can go through many more than CPUs in a given time. By relying on its 299,008 CPU cores to guide simulations and allowing its new NVIDIA GPUs to do the heavy lifting, Titan will enable researchers to run scientific calculations with greater speed and accuracy."Titan will allow scientists to simulate physical systems more realistically and in far greater detail," said James Hack, director of ORNL's National Center for Computational Sciences. "The improvements in simulation fidelity will accelerate progress in a wide range of research areas such as alternative energy and energy efficiency, the identification and development of novel and useful materials and the opportunity for more advanced climate projections."Titan will be open to select projects while ORNL and Cray work through the process for final system acceptance. The lion's share of access to Titan in the coming year will come from the Department of Energy's Innovative and Novel Computational Impact on Theory and Experiment program, better known as INCITE.Researchers have been preparing for Titan and its hybrid architecture for the past two years, with many ready to make the most of the system on day one. Among the flagship scientific applications on Titan:Materials Science The magnetic properties of materials hold the key to major advances in technology. The application WL-LSMS provides a nanoscale analysis of important materials such as steels, iron-nickel alloys and advanced permanent magnets that will help drive future electric motors and generators. Titan will allow researchers to improve the calculations of a material's magnetic states as they vary by temperature."The order-of-magnitude increase in computational power available with Titan will allow us to investigate even more realistic models with better accuracy," noted ORNL researcher and WL-LSMS developer Markus Eisenbach.Combustion The S3D application models the underlying turbulent combustion of fuels in an internal combustion engine. This line of research is critical to the American energy economy, given that three-quarters of the fossil fuel used in the United States goes to powering cars and trucks, which produce one-quarter of the country's greenhouse gases.Titan will allow researchers to model large-molecule hydrocarbon fuels such as the gasoline surrogate isooctane; commercially important oxygenated alcohols such as ethanol and butanol; and biofuel surrogates that blend methyl butanoate, methyl decanoate and n-heptane."In particular, these simulations will enable us to understand the complexities associated with strong coupling between fuel chemistry and turbulence at low preignition temperatures," noted team member Jacqueline Chen of Sandia National Laboratories. "These complexities pose challenges, but also opportunities, as the strong sensitivities to both the fuel chemistry and to the fluid flows provide multiple control options which may lead to the design of a high-efficiency, low-emission, optimally combined engine-fuel system."Nuclear Energy Nuclear researchers use the Denovo application to, among other things, model the behavior of neutrons in a nuclear power reactor. America's aging nuclear power plants provide about a fifth of the country's electricity, and Denovo will help them extend their operating lives while ensuring safety. Titan will allow Denovo to simulate a fuel rod through one round of use in a reactor core in 13 hours; this job took 60 hours on the Jaguar system.Climate Change The Community Atmosphere Model-Spectral Element simulates long-term global climate. Improved atmospheric modeling under Titan will help researchers better understand future air quality as well as the effect of particles suspended in the air.Using a grid of 14-kilometer cells, the new system will be able to simulate from one to five years per day of computing time, up from the three months or so that Jaguar was able to churn through in a day."As scientists are asked to answer not only whether the climate is changing but where and how, the workload for global climate models must grow dramatically," noted CAM-SE team member Kate Evans of ORNL. "Titan will help us address the complexity that will be required in such models."ORNL is managed by UT-Battelle for the Department of Energy. The Department of Energy is the single largest supporter of basic research in the physical sciences in the United States, and is working to address some of the most pressing challenges of our time. For more information, please visit http://science.energy.gov/.For more information, including Titan images and videos, please visit http://www.olcf.ornl.gov/titan/.This video demonstrates how a Barrett WAM arm uses our mixture of motor primitives (MoMP) algorithm to learn successful hitting movements in table tennis using imitation and reinforcement Learning.Anthony here. Just wanted to share a personal update now that I have a moment. Â At around 9:00 this morning upon hearing about the fuel situation at Peer1, I decided to head out and see if I could lend a hand. The streets of Manhattan near where I live (Soho â€“ not in the evacuation zones) are in not-so-bad shape right now, but the damage left from the flooding in the evacuation zones is significant and real.Iâ€™m sitting in our datacenter NOC at 75 Broad St. Not that itâ€™s been pointed out to me, but there are beds set up on the tiled floor here from the great team at Peer1 who stayed to monitor the situation overnight. These guys have incredible commitment to keeping everything running, and itâ€™s great to see.Normally, power loss would not be a major problem for our datacenter â€“ Peer1 stayed online during the major Manhattan power outage in 2003 that lasted for days, and we preemptively shifted to backup power around 4:00pm yesterday predicting that Con Edison would be shutting off power in evacuation zones. Â Given the nature of the flooding, this situation escalated greatly, submerging our reserve fuel in the basement, shutting off the elevators, and damaging the pumps required to get this fuel to the generator on the 17th floor.My reasons for coming to the datacenter were twofold: one was simply to help and do whatever I could to help us (and the whole building) stay online. The other was to send our systems team the absolute final signal to perform a clean shutdown of our infrastructure should we be moments away from total power loss. Generally, clean shutdowns are preferable to abrupt halts, since code halting in a known state is better than code halting in an unknown state.We had an initial warning that 10:45am was going to be the clean shutdown time. To determine how much time we have remaining, engineers are taking readings at particular time intervals to attempt to determine how quickly we are depleting. The tank readings are behaving somewhatÂ erratically, as there is another mechanism replenishing it from a separate fuel header. Some of our recent readings seem more optimistic, but it is impossible to predict how much fuel remains in this header at this time. As of this writing, we have at least 45 minutes left.Bridges to the island are open right now, and we currently have a fuel truck en route. We have approval from the building to manually carry fuel up in plastic water bottles, and we have a number of our team on-site to carry fuel up the stairs as needed. I do not know if the manual plan will be successful, but we will certainly try.Unfortunately, I do not have more information on a final resolution to this issue. You should still expect Squarespace to go offline at some point because of the hurricaneâ€™s aftermath, but we will do our best to keep that downtime to a minimum. Once we have a reliable stream of fuel to the building, it will go online independently of any other grid issues related to ConEdison and lower Manhattan in general.Weâ€™ll continue to keep you posted. Thank you for your patience.This video demonstrates how a Barrett WAM arm uses our mixture of motor primitives (MoMP) algorithm to learn successful hitting movements in table tennis using imitation and reinforcement Learning.MENLO PARK, Calif. â€” Many people cite Albert Einsteinâ€™s aphorism â€œEverything should be made as simple as possible, but no simpler.â€? Only a handful, however, have had the opportunity to discuss the concept with the physicist over breakfast.One of those is Peter G. Neumann, now an 80-year-old computer scientist at SRI International, a pioneering engineering research laboratory here.As an applied-mathematics student at Harvard, Dr. Neumann had a two-hour breakfast with Einstein on Nov. 8, 1952. What the young math student took away was a deeply held philosophy of design that has remained with him for six decades and has been his governing principle of computing and computer security.For many of those years, Dr. Neumann (pronounced NOY-man) has remained a voice in the wilderness, tirelessly pointing out that the computer industry has a penchant for repeating the mistakes of the past. He has long been one of the nationâ€™s leading specialists in computer security, and early on he predicted that the security flaws that have accompanied the pell-mell explosion of the computer and Internet industries would have disastrous consequences.â€œHis biggest contribution is to stress the â€˜systemsâ€™ nature of the security and reliability problems,â€? said Steven M. Bellovin, chief technology officer of the Federal Trade Commission. â€œThat is, trouble occurs not because of one failure, but because of the way many different pieces interact.â€?Dr. Bellovin said that it was Dr. Neumann who originally gave him the insight that â€œcomplex systems break in complex waysâ€? â€” that the increasing complexity of modern hardware and software has made it virtually impossible to identify the flaws and vulnerabilities in computer systems and ensure that they are secure and trustworthy.The consequence has come to pass in the form of an epidemic of computer malware and rising concerns about cyberwarfare as a threat to global security, voiced alarmingly this month by the defense secretary, Leon E. Panetta, who warned of a possible â€œcyber-Pearl Harborâ€? attack on the United States.It is remarkable, then, that years after most of his contemporaries have retired, Dr. Neumann is still at it and has seized the opportunity to start over and redesign computers and software from a â€œclean slate.â€?He is leading a team of researchers in an effort to completely rethink how to make computers and networks secure, in a five-year project financed by the Pentagonâ€™s Defense Advanced Research Projects Agency, or Darpa, with Robert N. Watson, a computer security researcher at Cambridge Universityâ€™s Computer Laboratory.â€œIâ€™ve been tilting at the same windmills for basically 40 years,â€? said Dr. Neumann recently during a lunchtime interview at a Chinese restaurant near his art-filled home in Palo Alto, Calif. â€œAnd I get the impression that most of the folks who are responsible donâ€™t want to hear about complexity. They are interested in quick and dirty solutions.â€?Dr. Neumann, who left Bell Labs and moved to California as a single father with three young children in 1970, has occupied the same office at SRI for four decades. Until the building was recently modified to make it earthquake-resistant, the office had attained notoriety for the towering stacks of computer science literature that filled every cranny. Legend has it that colleagues who visited the office after the 1989 earthquake were stunned to discover that while other offices were in disarray from the 7.1-magnitude quake, nothing in Dr. Neumannâ€™s office appeared to have been disturbed.A trim and agile man, with piercing eyes and a salt-and-pepper beard, Dr. Neumann has practiced tai chi for decades. But his passion, besides computer security, is music. He plays a variety of instruments, including bassoon, French horn, trombone and piano, and is active in a variety of musical groups. At computer security conferences it has become a tradition for Dr. Neumann to lead his colleagues in song, playing tunes from Gilbert and Sullivan and Tom Lehrer.Until recently, security was a backwater in the world of computing. Today it is a multibillion-dollar industry, though one of dubious competence, and safeguarding the nationâ€™s computerized critical infrastructure has taken on added urgency. President Obama cited it in the third debate of the presidential campaign, focusing on foreign policy, as something â€œwe need to be thinking aboutâ€? as part of the nationâ€™s military strategy.Dow Jones Reprints: This copy is for your personal, non-commercial use only. To order presentation-ready copies for distribution to your colleagues, clients or customers, use the Order Reprints tool at the bottom of any article or visit www.djreprints.comApple Inc. executive Scott Forstall was asked to leave the company after he refused to sign his name to a letter apologizing for shortcomings in Apple's new mapping service, according to people familiar with the matter.The incident was the latest clash between Mr. Forstall, who oversaw Apple's mobile software unit, and other executives at the company. It led to one of the most significant management shake-ups in Apple's recent history and its most sweeping changes under Chief Executive Tim Cook.Apple announced the departure of Mr. Forstall on Monday along with the unrelated departure of its new retail chief, ...Google announced a handful of new Nexus-branded products Monday, including the Nexus 4 smartphone, and the Nexus 7 and Nexus 10 tablets. With its new lineup of Nexus gear, Google is prepared to battle Apple for consumers' holiday dollars over the coming months.The Nexus 4 is in fact a rebadged version of LG's Optimus G. It's a fine smartphone, and perhaps the best ever made by LG. Its best features are the incredible 1280 x 768 HD, 4.7-inch display; quad-core Snapdragon processor; and killer 8-megapixel camera. It is going to be sold unlocked, without carrier contracts for the extremely low price of $299. It can be purchased directly from Google starting November 13.The Nexus 7 is a new version of the Asus-made Nexus 7 that's been available since June. Really the only thing that's different is the amount of storage available and the price. Google upped the possible max storage to 32 GB. The 16-GB Wi-Fi version costs $199, the 32-GB Wi-Fi version costs $249 and the 32-GB Wi-Fi and HSPA+ costs $299.The Nexus 10 is a brand new tablet manufactured from Apple-foe Samsung. There should be no doubt in anyone's mind that Samsung is hoping to appeal to consumers who lust after the Retina Display on Apple's iPad. The iPad 3 and 4 have 9.7-inch displays with 2048 x 1536 pixels and 264 pixels per inch. The Nexus 10 has a 10.05-inch display that has a 2560 x 1600 pixel resolution, making for 300 pixels per inch. Neither Google nor Samsung said what kind of technology is behind the display. Samsung typically favors AMOLEDs, while Apple favors LCDs.[ Smartphones and tablets are little without apps. Check out the 10 Best Apps For Samsung Galaxy Notes. ]In addition to the Retina Display-killing screen, the Nexus 10 boasts a dual-core A15 processor, and Mali T604 graphics processor with 2 GB of RAM; 5-megapixel main camera and 1.9-megapixel user-facing camera; 802.11b/g/n Wi-Fi, Bluetooth and NFC; and microUSB and HDMI ports.The Nexus 10 is priced fairly aggressively. The 16-GB version costs $399 and the 32-GB version costs $499. Neither offers 3G or 4G cellular data, though.It will be interesting to see how Samsung and Google market the Nexus 10. If there's one feature of the iPad 3 and iPad 4 that Apple likes to brag about, it's the Retina Display. Now that Samsung has a tablet with a higher-resolution display than the iPad, the mudslinging between the two competitors is probably going to get worse.All of the new Nexus devices run Android 4.2 Jelly Bean. This minor update to Android includes some pretty cool features, such as Photo Sphere. Photo Sphere lets people take 360-degree panoramas to create really wild images. It also adds a Swype-like keyboard and new powers for Google Now.The Nexus 7 and Nexus 10 can also be ordered directly from Google beginning November 13.Time to patch your security policy to address people bringing their own mobile devices to work. Also in the new Holes In BYOD issue of Dark Reading: Metasploit creator HD Moore has five practical security tips for business travelers. (Free registration required.)Google has officially announced the Nexus 4, the latest phone in its Nexus line of flagship Android devices. Built by LG, the phone features a 4.7-inch 1280 x 768 IPS display, a 1.5GHz quad-core Snapdragon S4 Pro processor â€” which Google claims is the fastest on the market â€” an 8 megapixel camera and a 1.3 megapixel front-facing camera, and up to 16GB of storage. Oh, and the back is made of glass â€” etched, layered glass that sparkles with a strange, almost holographic depth.The executive vibe is balanced nicely by the playfulness of the backNot much of that should be surprising, as the phone had been thoroughly leaked around the web in the past few weeks. What is surprising is how much better it all looks in person. Compared to the LG Optimus G, which shares many of the same components, it's no contest â€” the Nexus 4 is a far nicer piece of hardware. It feels weighty and high-end, and the tight construction combined with the soft-touch plastic on the sides and chrome edging give it a solidly executive vibe â€” a vibe that's balanced nicely by the playfulness of Disco City on the back.The device will sell for $299 with 8GB of storage, or $349 with 16GB. A T-Mobile version will sell unlocked for $199 on a two-year contract. Alongside the improved screen and faster CPU, the Nexus 4 has 2GB of RAM, Wi-Fi 802.11b/g/n, NFC, Bluetooth, and built-in compatibility with Google's latest accessory, the Wireless Charging Orb â€” an inductive charging dock. The phone also houses a sizable 2100 mAh battery, which the company claims will get you about 10 hours of talk time.There's no LTE hereAll that battery life would be great if the device was sporting LTE radios â€” but it is not. Google has decided to forgo stricter carrier partnerships in the US, which for now means that the company will only offer the device as an unlocked HSPA+ phone. That's a bit of a crushing blow to many, who expected Google's next flagship phone to go toe-to-toe with the iPhone 5 and the latest crop of Windows Phone devices.On the bright side, the 320 ppi IPS+ LCD screen is terrific â€” a massive upgrade over the so-so Galaxy Nexus display and competitive with the iPhone's 326 ppi Retina Display. And it's not just competitive in pixel density; the screen looks stunning. It's also laminated and uses LG's new "G2" technology which integrates the touch sensor into the Gorilla Glass 2 outer layer, making everything thinner as well as bringing the actual pixels closer to the surface of the display. (Apple uses a similar technique called "in-cell touch" on the iPhone 5, which integrates the touch sensor into the display panel.) The screen is also curved slightly at the edges, like it's been melted over the phone; Google says it's meant to improve swiping in from the sides of the device.Performance on the phone was snappy. Google execs we spoke with pointed out just how fast the new Snapdragon CPU is, and in our short time testing the phone, it seemed to rip through just about anything we threw at it with little or no hesitation.The screen is curved slightly at the edges, like it's been melted over the phoneFor those disappointed with the camera performance of the Galaxy Nexus, there's also a bright spot here. Literally. Photos taken with the Nexus 4 seem greatly improved over the last generation, and Google reps say that a lot of attention has been paid to the low-light performance of the camera. We won't know for sure just how much better it is than previous phones until we put the device through its full paces, but first impressions suggest a big improvement.On the software front, Google is launching Android 4.2 along with the Nexus 4 (and the Nexus 10 tablet), and it's got some killer new features. We have a full look at the software here, not to mention an exclusive feature on the inside story of the Android team here, but there are a few standout components of the OS update that are worth mentioning.For starters, Google has added widget functionality to the lock screen, meaning you can quickly glance at information without having to get into the phone. The camera has also been improved with a completely redesigned UI focused on single-handed input, and Google has added a Street View-like mode called Photo Sphere which makes panorama shots seem tiny by comparison. The company has also improved Google Now significantly (we have a big feature story on that too).Android now has a typing mode called Gesture Typing, which mimics the functionality of Swype in conjunction with standard tap typing. The company has also added a new quick settings menu to the notifications window, tweaked Gmail with much-needed features like swipe to archive and scale-to-fit messages (like the iPhone), and added new accessibility options that make Android easier than ever â€” for all users.We'll have a full review of the Nexus 4 soon; until then, be sure to check into all of the in-depth news on Google's announcements today.You can also watch this video on YouTube.Supercomputer combines gaming and traditional computing technologies to provide unprecedented power for researchOAK RIDGE, Tenn. â€“ The U.S. Department of Energy's (DOE) Oak Ridge National Laboratory launched a new era of scientific supercomputing today with Titan, a system capable of churning through more than 20,000 trillion calculations each second-or 20 petaflops-by employing a family of processors called graphic processing units first created for computer gaming. Titan will be 10 times more powerful than ORNL's last world-leading system, Jaguar, while overcoming power and space limitations inherent in the previous generation of high-performance computers.Titan, which is supported by the Department of Energy, will provide unprecedented computing power for research in energy, climate change, efficient engines, materials and other disciplines and pave the way for a wide range of achievements in science and technology.The Cray XK7 system contains 18,688 nodes, with each holding a 16-core AMD Opteron 6274 processor and an NVIDIA Tesla K20 graphics processing unit (GPU) accelerator. Titan also has more than 700 terabytes of memory. The combination of central processing units, the traditional foundation of high- performance computers, and more recent GPUs will allow Titan to occupy the same space as its Jaguar predecessor while using only marginally more electricity."One challenge in supercomputers today is power consumption," said Jeff Nichols, associate laboratory director for computing and computational sciences. "Combining GPUs and CPUs in a single system requires less power than CPUs alone and is a responsible move toward lowering our carbon footprint. Titan will provide unprecedented computing power for research in energy, climate change, materials and other disciplines to enable scientific leadership."Because they handle hundreds of calculations simultaneously, GPUs can go through many more than CPUs in a given time. By relying on its 299,008 CPU cores to guide simulations and allowing its new NVIDIA GPUs to do the heavy lifting, Titan will enable researchers to run scientific calculations with greater speed and accuracy."Titan will allow scientists to simulate physical systems more realistically and in far greater detail," said James Hack, director of ORNL's National Center for Computational Sciences. "The improvements in simulation fidelity will accelerate progress in a wide range of research areas such as alternative energy and energy efficiency, the identification and development of novel and useful materials and the opportunity for more advanced climate projections"Titan will be open to select projects while ORNL and Cray work through the process for final system acceptance. The lion's share of access to Titan in the coming year will come from the Department of Energy's Innovative and Novel Computational Impact on Theory and Experiment program, better known as INCITE.Researchers have been preparing for Titan and its hybrid architecture for the past two years, with many ready to make the most of the system on day one. Among the flagship scientific applications on Titan:Materials Science The magnetic properties of materials hold the key to major advances in technology. The application WL- LSMS provides a nanoscale analysis of important materials such as steels, iron-nickel alloys and advanced permanent magnets that will help drive future electric motors and generators. Titan will allow researchers to improve the calculations of a material's magnetic states as they vary by temperature."The order-of-magnitude increase in computational power available with Titan will allow us to investigate even more realistic models with better accuracy" noted ORNL researcher and WL-LSMS developer Markus Eisenbach. CombustionThe S3D application models the underlying turbulent combustion of fuels in an internal combustion engine. This line of research is critical to the American energy economy, given that three-quarters of the fossil fuel used in the United States goes to powering cars and trucks, which produce one-quarter of the country's greenhouse gases.Titan will allow researchers to model large-molecule hydrocarbon fuels such as the gasoline surrogate isooctane; commercially important oxygenated alcohols such as ethanol and butanol; and biofuel surrogates that blend methyl butanoate, methyl decanoate and n-heptane."In particular, these simulations will enable us to understand the complexities associated with strong coupling between fuel chemistry and turbulence at low preignition temperatures," noted team member Jacqueline Chen of Sandia National Laboratories. "These complexities pose challenges, but also opportunities, as the strong sensitivities to both the fuel chemistry and to the fluid flows provide multiple control options which may lead to the design of a high-efficiency, low-emission, optimally combined engine-fuel system"Nuclear Energy Nuclear researchers use the Denovo application to, among other things, model the behavior of neutrons in a nuclear power reactor. America's aging nuclear power plants provide about a fifth of the country's electricity, and Denovo will help them extend their operating lives while ensuring safety. Titan will allow Denovo to simulate a fuel rod through one round of use in a reactor core in 13 hours; this job took 60 hours on the Jaguar system.Climate Change The Community Atmosphere Modelâ€“Spectral Element simulates long-term global climate. Improved atmospheric modeling under Titan will help researchers better understand future air quality as well as the effect of particles suspended in the air.Using a grid of 14-kilometer cells, the new system will be able to simulate from one to five years per day of computing time, up from the three months or so that Jaguar was able to churn through in a day."As scientists are asked to answer not only whether the climate is changing but where and how, the workload for global climate models must grow dramatically," noted CAM-SE team member Kate Evans of ORNL. "Titan will help us address the complexity that will be required in such models."ORNL is managed by UT-Battelle for the Department of Energy. The Department of Energy is the single largest supporter of basic research in the physical sciences in the United States, and is working to address some of the most pressing challenges of our time. For more information, please visit http://science.energy.gov.Look out, silicon. IBM has managed to create a computer chip based on newer carbon-nanotube technology with more than 10,000 transistors. While thatâ€™s still a drop in the bucket compared to the billions of transistors on todayâ€™s state-of-the-art silicon microprocessors, itâ€™s an important step in proving the viability of the new tech.You may have heard of Mooreâ€™s Law, which says that the number of transistors that can be put in a computer chip doubles every 18 months. That â€œlawâ€? has held true for four decades, successfully predicting the rapid evolution of computers and smartphones.However, itâ€™s not a law like, say, Boyleâ€™s Law, which is an inviolable tenet of physics. Mooreâ€™s Law is just a prediction, and itâ€™s about to collide with those real physical laws in the next few years as transistors approach the limit of how small they can get. What then?IBM has an answer in the form of a relatively new technology: carbon nanotubes. Each tube is an atom thick, rolled up in a cylinder (one is shown above). Carbon nanotubes actually conduct electricity better than silicon, have the perfect shape to act as a transistor and, most importantly, can scale smaller.However, theyâ€™re also much harder to work with, which is why no oneâ€™s pursued the tech until recently. They have to be aligned perfectly and metallic impurities, which naturally occur, must be completely removed.IBM has met those challenges, however, not only creating a 10,000-transistor-strong processor based on carbon nanotubes, but doing it with standard semiconductor techniques. That means, should todayâ€™s chipmakers end up switching to the technology, they wouldnâ€™t have to spend billions creating new tools and production facilities.It would also mean Mooreâ€™s Law could get a new lease on life, just through a different technology. And the gadget market, which has been reliant on introducing newer and more powerful gadgets year after year, should be safe for another decade at least.Hackers penetrated the computer defenses of South Carolina's Department of Revenue and accessed 3.6 million social security numbers and account data for 387,000 payment cards, officials said. The Associated Press reported the intrusion also exposed citizens' tax returns, which typically contain much more sensitive personal information, but that couldn't immediately be confirmed.The breach, which occurred in mid-September, followed a series of attempted intrusions beginning in August, according to a press release. State officials have known of the data breach since October 16, and suspected an intrusion as early as October 10, but didn't disclose it until Friday, just hours before the start of the weekend. The underlying vulnerability that attackers exploited to access the state network was fixed on October 20.Officials have retained security firm Mandiant to assist in the investigation of the breach and to help secure the system. The state is also offering one year of credit-monitoring and identity-theft protection from Experian."The number of records breached requires an unprecedented, large-scale response by the Department of Revenue, the State of South Carolina and all our citizens," Governor Nikki Haley was quoted as saying in the press release. "We are taking immediate steps to protect the taxpayers of South Carolina, including providing one year of credit monitoring of identity protection to those affected."Of the 387,000 payment cards exposed, all but 16,000 were encrypted using measures "deemed sufficient" under credit card industry standards, presumably a reference to the Payment Card Industry Data Security Standard, which critics say doesn't go far enough in protecting account data. With a state population of about 4.6 million, the exposure could affect as many as much as three-fourths of South Carolina citizens.Malware samples use increasingly refined trickery to avoid being detected by automated threat analysis systems. Anti-virus company Symantec reports that it has found a trojan which attaches its malicious code to the routines for handling mouse events. Since nobody moves the mou